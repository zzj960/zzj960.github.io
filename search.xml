<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[内存管理]]></title>
    <url>%2F2019%2F06%2F08%2F%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[内存管理包括内存管理和虚拟内存管理。 内存管理包括:内存管理概念、交换与覆盖、连续分配管理方式和非连续分配管理方式（分页管理方式、分段管理方式、段页式管理方式）。虚拟内存管理包括:虚拟内存概念、请求分页管理方式、页面置换算法、页面分配策略、工作集和抖动。 内存管理的概念内存管理(Memory Management)是操作系统设计中最重要和最复杂的内容之一。虽然计算机硬件一直在飞速发展，内存容量也在不断增长，但是仍然不可能将所有用户进程和系统所需要的全部程序和数据放入主存中，所以操作系统必须将内存空间进行合理地划分和有效地动态分配。操作系统对内存的划分和动态分配，就是内存管理的概念。 有效的内存管理在多道程序设计中非常重要，不仅方便用户使用存储器、提高内存利用率，还可以通过虚拟技术从逻辑上扩充存储器。 内存管理的功能有： 内存空间的分配与回收：由操作系统完成主存储器空间的分配和管理，使程序员摆脱存储分配的麻烦，提高编程效率。 地址转换：在多道程序环境下，程序中的逻辑地址与内存中的物理地址不可能一致，因此存储管理必须提供地址变换功能，把逻辑地址转换成相应的物理地址。 内存空间的扩充：利用虚拟存储技术或自动覆盖技术，从逻辑上扩充内存。 存储保护：保证各道作业在各自的存储空间内运行，.互不干扰。 在进行具体的内存管理之前，需要了解进程运行的基本原理和要求。 程序装入和链接创建进程首先要将程序和数据装入内存。将用户源程序变为可在内存中执行的程序，通常需要以下几个步骤： 编译：由编译程序将用户源代码编译成若干个目标模块。 链接：由链接程序将编译后形成的一组目标模块，以及所需库函数链接在一起，形成一个完整的装入模块。 装入：由装入程序将装入模块装入内存运行。 程序的链接有以下三种方式： 静态链接：在程序运行之前，先将各目标模块及它们所需的库函数链接成一个完整的可执行程序，以后不再拆开。 装入时动态链接：将用户源程序编译后所得到的一组目标模块，在装入内存时，釆用边装入边链接的链接方式。 运行时动态链接：对某些目标模块的链接，是在程序执行中需要该目标模块时，才对它进行的链接。其优点是便于修改和更新，便于实现对目标模块的共享。 内存的装入模块在装入内存时，同样有以下三种方式： 1.绝对装入。在编译时，如果知道程序将驻留在内存的某个位置，编译程序将产生绝对地址的目标代码。绝对装入程序按照装入模块中的地址，将程序和数据装入内存。由于程序中的逻辑地址与实际内存地址完全相同，故不需对程序和数据的地址进行修改。 绝对装入方式只适用于单道程序环境。另外，程序中所使用的绝对地址,可在编译或汇编时给出，也可由程序员直接赋予。而通常情况下在程序中釆用的是符号地址，编译或汇编时再转换为绝对地址。 2.可重定位装入。在多道程序环境下，多个目标模块的起始地址通常都是从0开始，程序中的其他地址都是相对于起始地址的,此时应釆用可重定位装入方式。根据内存的当前情况，将装入模块装入到内存的适当位置。装入时对目标程序中指令和数据的修改过程称为重定位，地址变换通常是在装入时一次完成的，所以又称为静态重定位 静态重定位的特点是在一个作业装入内存时，必须分配其要求的全部内存空间，如果没有足够的内存，就不能装入该作业。此外，作业一旦进入内存后，在整个运行期间不能在内存中移动，也不能再申请内存空间。 3.动态运行时装入，也称为动态重定位，程序在内存中如果发生移动，就需要釆用动态的装入方式。装入程序在把装入模块装入内存后，并不立即把装入模块中的相对地址转换为绝对地址，而是把这种地址转换推迟到程序真正要执行时才进行。因此，装入内存后的所有地址均为相对地址。这种方式需要一个重定位寄存器的支持 动态重定位的特点是可以将程序分配到不连续的存储区中；在程序运行之前可以只装入它的部分代码即可投入运行，然后在程序运行期间，根据需要动态申请分配内存；便于程序段的共享，可以向用户提供一个比存储空间大得多的地址空间。 逻辑地址空间与物理地址空间编译后，每个目标模块都是从0号单元开始编址，称为该目标模块的相对地址（或逻辑地址)。当链接程序将各个模块链接成一个完整的可执行目标程序时，链接程序顺序依次按各个模块的相对地址构成统一的从0号单元开始编址的逻辑地址空间。用户程序和程序员只需知道逻辑地址，而内存管理的具体机制则是完全透明的，它们只有系统编程人员才会涉及。不同进程可以有相同的逻辑地址，因为这些相同的逻辑地址可以映射到主存的不同位置。 物理地址空间是指内存中物理单元的集合，它是地址转换的最终地址，进程在运行时执行指令和访问数据最后都要通过物理地址从主存中存取。当装入程序将可执行代码装入内存时，必须通过地址转换将逻辑地址转换成物理地址，这个过程称为地址重定位。 内存保护内存分配前，需要保护操作系统不受用户进程的影响，同时保护用户进程不受其他用户进程的影响。通过釆用重定位寄存器和界地址寄存器来实现这种保护。重定位寄存器含最小的物理地址值，界地址寄存器含逻辑地址值。每个逻辑地址值必须小于界地址寄存器；内存管理机构动态地将逻辑地址与界地址寄存器进行比较，如果未发生地址越界，则加上重定位寄存器的值后映射成物理地址，再送交内存单元，如图3-3所示。 当CPU调度程序选择进程执行时，派遣程序会初始化重定位寄存器和界地址寄存器。每一个逻辑地址都需要与这两个寄存器进行核对，以保证操作系统和其他用户程序及数据不被该进程的运行所影响。 内存覆盖与内存交换覆盖与交换技术是在多道程序环境下用来扩充内存的两种方法。 内存覆盖早期的计算机系统中，主存容量很小，虽然主存中仅存放一道用户程序，但是存储空间放不下用户进程的现象也经常发生，这一矛盾可以用覆盖技术来解决。 覆盖的基本思想是：由于程序运行时并非任何时候都要访问程序及数据的各个部分（尤其是大程序），因此可以把用户空间分成一个固定区和若干个覆盖区。将经常活跃的部分放在固定区，其余部分按调用关系分段。首先将那些即将要访问的段放入覆盖区，其他段放在外存中，在需要调用前，系统再将其调入覆盖区，替换覆盖区中原有的段。 覆盖技术的特点是打破了必须将一个进程的全部信息装入主存后才能运行的限制，但当同时运行程序的代码量大于主存时仍不能运行。 内存交换交换（对换）的基本思想是，把处于等待状态（或在CPU调度原则下被剥夺运行权利）的程序从内存移到辅存，把内存空间腾出来，这一过程又叫换出；把准备好竞争CPU运行的程序从辅存移到内存，这一过程又称为换入。中级调度就是釆用交换技术。 例如，有一个CPU釆用时间片轮转调度算法的多道程序环境。时间片到，内存管理器将刚刚执行过的进程换出，将另一进程换入到刚刚释放的内存空间中。同时，CPU调度器可以将时间片分配给其他已在内存中的进程。每个进程用完时间片都与另一进程交换。理想情况下，内存管理器的交换过程速度足够快，总有进程在内存中可以执行。 有关交换需要注意以下几个问题： 交换需要备份存储，通常是快速磁盘。它必须足够大，并且提供对这些内存映像的直接访问。 为了有效使用CPU，需要每个进程的执行时间比交换时间长，而影响交换时间的主要是转移时间。转移时间与所交换的内存空间成正比。 如果换出进程，必须确保该进程是完全处于空闲状态。 交换空间通常作为磁盘的一整块，且独立于文件系统，因此使用就可能很快。 交换通常在有许多进程运行且内存空间吃紧时开始启动，而系统负荷降低就暂停。 普通的交换使用不多，但交换策略的某些变种在许多系统中（如UNIX系统）仍发挥作用。 交换技术主要是在不同进程（或作业）之间进行，而覆盖则用于同一个程序或进程中。由于覆盖技术要求给出程序段之间的覆盖结构，使得其对用户和程序员不透明，所以对于主存无法存放用户程序的矛盾，现代操作系统是通过虚拟内存技术来解决的，覆盖技术则已成为历史；而交换技术在现代操作系统中仍具有较强的生命力。 内存连续分配管理方式连续分配方式，是指为一个用户程序分配一个连续的内存空间。它主要包括单一连续分配、固定分区分配和动态分区分配。 单一连续分配内存在此方式下分为系统区和用户区，系统区仅提供给操作系统使用，通常在低地址部分；用户区是为用户提供的、除系统区之外的内存空间。这种方式无需进行内存保护。 这种方式的优点是简单、无外部碎片，可以釆用覆盖技术，不需要额外的技术支持。缺点是只能用于单用户、单任务的操作系统中，有内部碎片，存储器的利用率极低。 固定分区分配固定分区分配是最简单的一种多道程序存储管理方式，它将用户内存空间划分为若干个固定大小的区域，每个分区只装入一道作业。当有空闲分区时，便可以再从外存的后备作业队列中,选择适当大小的作业装入该分区，如此循环。 固定分区分配在划分分区时，有两种不同的方法 分区大小相等：用于利用一台计算机去控制多个相同对象的场合，缺乏灵活性。 分区大小不等：划分为含有多个较小的分区、适量的中等分区及少量的大分区。 为便于内存分配，通常将分区按大小排队，并为之建立一张分区说明表，其中各表项包括每个分区的起始地址、大小及状态（是否已分配）。当有用户程序要装入时，便检索该表，以找到合适的分区给予分配并将其状态置为”已分配”；未找到合适分区则拒绝为该用户程序分配内存。 这种分区方式存在两个问题：一是程序可能太大而放不进任何一个分区中，这时用户不得不使用覆盖技术来使用内存空间；二是主存利用率低，当程序小于固定分区大小时，也占用了一个完整的内存分区空间，这样分区内部有空间浪费，这种现象称为内部碎片。 固定分区是可用于多道程序设计最简单的存储分配，无外部碎片，但不能实现多进程共享一个主存区，所以存储空间利用率低。固定分区分配很少用于现在通用的操作系统中，但在某些用于控制多个相同对象的控制系统中仍发挥着一定的作用。]]></content>
      <categories>
        <category>计算机</category>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>计算机</tag>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[死锁]]></title>
    <url>%2F2019%2F06%2F06%2F%E6%AD%BB%E9%94%81%2F</url>
    <content type="text"><![CDATA[死锁的定义在多道程序系统中，由于多个进程的并发执行，改善了系统资源的利用率并提高了系统的处理能力。然而，多个进程的并发执行也带来了新的问题——死锁。所谓死锁是指多个进程因竞争资源而造成的一种僵局（互相等待），若无外力作用，这些进程都将无法向前推进。 先看生活中的一个实例，在一条河上有一座桥，桥面很窄，只能容纳一辆汽车通行。如果有两辆汽车分别从桥的左右两端驶上该桥，则会出现下述的冲突情况。此时，左边的汽车占有了桥面左边的一段，要想过桥还需等待右边的汽车让出桥面右边的一段；右边的汽车占有了桥面右边的一段，要想过桥还需等待左边的汽车让出桥面左边的一段。此时，若左右两边的汽车都只能向前行驶，则两辆汽车都无法过桥。在计算机系统中也存在类似的情况。例如，某计算机系统中只有一台打印机和一台输入设备，进程P1正占用输入设备，同时又提出使用打印机的请求，但此时打印机正被进程P2 所占用，而P2在未释放打印机之前，又提出请求使用正被P1占用着的输入设备。这样两个进程相互无休止地等待下去，均无法继续执行，此时两个进程陷入死锁状态。 产生死锁的四大必要条件①资源互斥/资源不共享每个资源要么已经分配给了一个进程，要么是可用的，只有这两种状态，资源不可以被共享使用，所以所谓的互斥是指：资源不共享，如果被使用，只能被一个进程使用。 ②占有和等待/请求并保持已经得到资源的进程还能继续请求新的资源，所以个人觉得叫占有并请求也许更好理解。 ③资源不可剥夺当一个资源分配给了一个进程后，其它需要该资源的进程不能强制性获得该资源，除非该资源的当前占有者显示地释放该资源。 ④环路等待死锁发生时，系统中一定有由两个或两个以上的进程组成的一条环路，环路上的每个进程都在等待下一个进程所占有的资源。 解决死锁的方法 预防死锁（破坏产生死锁的条件） 避免死锁（银行家算法） 检测死锁（资源分配图） 解除死锁 第三条与第四条一般联合使用 防止死锁的方法 ①防止死锁的发生只需破坏死锁产生的四个必要条件之一即可。②下面的方法开销非常之大，目前没有一个操作系统可以实现。③因此，目前使用的方法是避免死锁，而不是防止死锁。④这部分的内容大致浏览简单了解一遍即可，只要能在某些选择题中判断出选项对应的是下面四个方法中的哪个就可以了。 破坏互斥条件方法：如果允许系统资源都能共享使用，则系统不会进入死锁状态。缺点：有些资源根本不能同时访问，如打印机等临界资源只能互斥使用。所以，破坏互斥条件而预防死锁的方法不太可行，而且在有的场合应该保护这种互斥性。 破坏请求并保持条件方法：釆用预先静态分配方法，即进程在运行前一次申请完它所需要的全部资源，在它的资源未满足前，不把它投入运行。一旦投入运行后，这些资源就一直归它所有，也不再提出其他资源请求，这样就可以保证系统不会发生死锁。缺点：系统资源被严重浪费，其中有些资源可能仅在运行初期或运行快结束时才使用，甚至根本不使用。而且还会导致“饥饿”现象，当由于个别资源长期被其他进程占用时，将致使等待该资源的进程迟迟不能开始运行。 破坏不可剥夺条件方法：当一个已保持了某些不可剥夺资源的进程，请求新的资源而得不到满足时，它必须释放已经保持的所有资源，待以后需要时再重新申请。这意味着，一个进程已占有的资源会被暂时释放，或者说是被剥夺了，或从而破坏了不可剥夺条件。缺点：该策略实现起来比较复杂，释放已获得的资源可能造成前一阶段工作的失效，反复地申请和释放资源会增加系统开销，降低系统吞吐量。这种方法常用于状态易于保存和恢复的资源，如CPU的寄存器及内存资源，一般不能用于打印机之类的资源。 破坏循环等待条件方法：为了破坏循环等待条件，可釆用顺序资源分配法。首先给系统中的资源编号，规定每个进程，必须按编号递增的顺序请求资源，同类资源一次申请完。也就是说，只要进程提出申请分配资源Ri，则该进程在以后的资源申请中，只能申请编号大于Ri的资源。缺点：这种方法存在的问题是，编号必须相对稳定，这就限制了新类型设备的增加；尽管在为资源编号时已考虑到大多数作业实际使用这些资源的顺序，但也经常会发生作业使用资源的顺序与系统规定顺序不同的情况，造成资源的浪费；此外，这种按规定次序申请资源的方法，也必然会给用户的编程带来麻烦。 避免死锁的算法判断“系统安全状态”法在进行系统资源分配之前，先计算此次资源分配的安全性。若此次分配不会导致系统进入不安全状态，则将资源分配给进程； 否则，让进程等待。 银行家算法1、申请的贷款额度不能超过银行现有的资金总额2、分批次向银行提款，但是贷款额度不能超过一开始最大需求量的总额3、暂时不能满足客户申请的资金额度时，在有限时间内给予贷款4、客户要在规定的时间内还款 死锁的检测 该部分讲述如何判断是否产生死锁 画出资源分配图系统死锁，可利用资源分配图来描述。如下图所示，用长方形代表一个进程，用框代表一类资源。由于一种类型的资源可能有多个，用框中的一个点代表一类资源中的一个资源。从进程到资源的有向边叫请求边，表示该进程申请一个单位的该类资源；从资源到进程的边叫分配边，表示该类资源已经有一个资源被分配给了该进程。 简化资源分配图第一步：先看A资源，它有三个箭头是向外的，因此它一共给进程分配了3个资源，此时，A没有空闲的资源剩余。第二步：再看B资源，它有一个箭头是向外的，因此它一共给进程分配了1个资源，此时，B还剩余一个空闲的资源没分配。第三步：看完资源，再来看进程，先看进程P2，它只申请一个A资源，但此时A资源已经用光了，所以，进程P2进入阻塞状态，因此，进程P2暂时不能化成孤立的点。第四步：再看进程P1，它只申请一个B资源，此时，系统还剩余一个B资源没分配，因此，可以满足P1的申请。这样，进程P1便得到了它的全部所需资源，所以它不会进入阻塞状态，可以一直运行，等它运行完后，我们再把它的所有的资源释放。相当于：可以把P1的所有的边去掉，变成一个孤立的点，如下图所示：第五步：进程P1运行完后，释放其所占有的资源（2个A资源和1个B资源），系统回收这些资源后，空闲的资源便变成2个A资源和1个B资源，由于进程P2一直在申请一个A资源，所以此时，系统能满足它的申请。这样，进程P2便得到了它的全部所需资源，所以它不会进入阻塞状态，可以一直运行，等它运行完后，我们再把它的所有的资源释放。相当于：可以把P2的所有的边都去掉，化成一个孤立的点，变成下图： 若能消去图中所有的边，则称该图是可完全简化的，如上图 使用死锁定理判断死锁定理：①如果资源分配图中没有环路，则系统没有死锁；②如果资源分配图中出现了环路，则系统可能有死锁。或者说：当且仅当S状态的资源分配图是不可完全简化的时候，系统状态则是死锁状态 死锁的解除1、资源剥夺法挂起某些死锁进程，并抢占它的资源，将这些资源分配给其他的死锁进程。但应防止被挂起的进程长时间得不到资源，而处于资源匮乏的状态。 2、撤销进程法强制撤销部分、甚至全部死锁进程并剥夺这些进程的资源。撤销的原则可以按进程优先级和撤销进程代价的高低进行。 3、进程回退法让一（多）个进程回退到足以回避死锁的地步，进程回退时自愿释放资源而不是被剥夺。要求系统保持进程的历史信息，设置还原点。]]></content>
      <categories>
        <category>计算机</category>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>计算机</tag>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程同步]]></title>
    <url>%2F2019%2F06%2F06%2F%E8%BF%9B%E7%A8%8B%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[进程同步的一些概念 互斥与同步临界资源（临界区）：指一次只能允许一个进程使用的共享资源称为临界资源； 同步：指为完成某种任务而建立的两个和多个进程，这些进程在合作的过程中需要协调工作次序进行有序的访问而出现等待所产生的制约关系。互斥：指两个或多个进程访问临界资源时只能一个进程访问，其他进程等待的一种相互制约的关系。 信号量与互斥量信号量：本身是一个计数器，使用P，V两个操作来实现计数的减与加，当计数不大于0时，则进程进入睡眠状态，它用于为多个进程提供共享数据对象的访问。互斥量：如果信号量只存在两个状态，那就不需要计数了，可以简化为加锁与解锁两个功能，这就是互斥量。 进程同步的四种方法临界区（Critical Section）通过对多线程的串行化来访问公共资源或一段代码，速度快，适合控制数据访问。 优点：保证在某一时刻只有一个线程能访问数据的简便办法缺点：虽然临界区同步速度很快，但却只能用来同步本进程内的线程，而不可用来同步多个进程中的线程。 互斥量（Mutex）为协调共同对一个共享资源的单独访问而设计的。 互斥量跟临界区很相似，比临界区复杂，互斥对象只有一个，只有拥有互斥对象的线程才具有访问资源的权限。 优点：使用互斥不仅仅能够在同一应用程序不同线程中实现资源的安全共享，而且可以在不同应用程序的线程之间实现对资源的安全共享。缺点： ①互斥量是可以命名的，也就是说它可以跨越进程使用，所以创建互斥量需要的资源更多，所以如果只为了在进程内部是用的话使用临界区会带来速度上的优势并能够减少资源占用量。因为互斥量是跨进程的互斥量一旦被创建，就可以通过名字打开它。②通过互斥量可以指定资源被独占的方式使用，但如果有下面一种情况通过互斥量就无法处理，比如现在一位用户购买了一份三个并发访问许可的数据库系统，可以根据用户购买的访问许可数量来决定有多少个线程/进程能同时进行数据库操作，这时候如果利用互斥量就没有办法完成这个要求，信号量对象可以说是一种资源计数器。 信号量（Semaphore）为控制一个具有有限数量用户资源而设计。它允许多个线程在同一时刻访问同一资源，但是需要限制在同一时刻访问此资源的最大线程数目。互斥量是信号量的一种特殊情况，当信号量的最大资源数=1就是互斥量了。 优点：适用于对Socket（套接字）程序中线程的同步。（例如，网络上的HTTP服务器要对同一时间内访问同一页面的用户数加以限制，只有不大于设定的最大用户数目的线程能够进行访问，而其他的访问企图则被挂起，只有在有用户退出对此页面的访问后才有可能进入。）缺点： ①信号量机制必须有公共内存，不能用于分布式操作系统，这是它最大的弱点；②信号量机制功能强大，但使用时对信号量的操作分散， 而且难以控制，读写和维护都很困难，加重了程序员的编码负担；③核心操作P-V分散在各用户程序的代码中，不易控制和管理，一旦错误，后果严重，且不易发现和纠正。 事件（Event）用来通知线程有一些事件已发生，从而启动后继任务的开始。 优点：事件对象通过通知操作的方式来保持线程的同步，并且可以实现不同进程中的线程同步操作。缺点： 总结①临界区不是内核对象，只能用于进程内部的线程同步，是用户方式的同步。互斥、信号量是内核对象可以用于不同进程之间的线程同步（跨进程同步）。②互斥其实是信号量的一种特殊形式。互斥可以保证在某一时刻只有一个线程可以拥有临界资源。信号量可以保证在某一时刻有指定数目的线程可以拥有临界资源。]]></content>
      <categories>
        <category>计算机</category>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>计算机</tag>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CPU调度]]></title>
    <url>%2F2019%2F06%2F04%2FCPU%E8%B0%83%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[CPU调度即按一定的调度算法从就绪队列中选择一个进程，把CPU的使用权交给被选中的进程，如果没有就绪进程，系统会安排一个系统空闲进程或idle进程。 CPU调度时机发生在内核对中断/异常/系统调用处理后返回到用户态时，具体来说有以下情况： 进程正常终止 或 由于某种错误而终止； 新进程创建 或 一个等待进程变成就绪； 当一个进程从运行态进入阻塞态； 当一个进程从运行态变为就绪态。 进程切换是指一个进程让出处理器，由另一个进程占用处理器的过程，包括以下两部分工作： 切换全局页目录以加载一个新的地址空间； 切换内核栈和硬件上下文，其中硬件上下文包括了内核执行新进程需要的全部信息，如CPU相关寄存器。 CPU调度算法衡量指标 吞吐量 （Throughput）： 每单位时间完成的进程数目； 周转时间TT (Turnaround Time)：每个进程从提出请求到运行完成的时间； 响应时间RT(Response Time)：从提出请求到第一次回应的时间； CPU利用率(CPU Utilization)：CPU做有效工作的时间比例； 等待时间(Waiting time)：每个进程在就绪队列(ready queue)中等待的时间； …… 批处理系统中采用的调度算法 衡量指标：吞吐量，周转时间，CPU利用率，公平平衡 先来先服务算法（FCFS——First Come First Serve）：按照进程就绪的先后顺序使用CPU。特点：非抢占，公平，实现简单，长进程后面的短进程需要等很长时间，不利于用户体验。 最短作业优先（SJF——Shortest Job First）：具有最短完成时间的进程优先执行，非抢占。最短剩余时间优先（SRTN——Shortest Remaining Time Next）：SJF抢占式版本，即当一个新就绪的进程比当前运行进程具有更短完成时间时，系统抢占当前进程，选择新就绪的进程执行。特点：有最短的平均周转时间，但不公平，源源不断的短任务到来，可能使长的任务长时间得不到运行，从而产生 “饥饿”现象 (starvation)。 最高响应比优先算法（HRRN——Highest Response Ratio Next）：是一个综合算法，调度时，首先计算每个进程的响应比R，之后总是选择R最高的进程执行。{响应比R = 周转时间 / 处理时间 =（处理时间 + 等待时间）/ 处理时间 = 1 +（等待时间 / 处理时间）}特点：折中权衡 交互式系统采用的调度算法 衡量指标：响应时间，公平平衡。 时间片轮转调度算法（Round Robin——RR）： 每个进程被分配一个时间片，允许该进程在该时间段运行，如果在时间片结束时该进程还在运行，则剥夺CPU并分配给另一个进程，如果该进程在时间片结RR束前阻塞或结束，则CPU立即进行切换。特点：公平；有利于交互式计算，响应时间快；由于进程切换，时间片轮转算法要花费较高的开销；对进程表中不同进程的大小差异较大的有利，而对进程都是相同大小的不利。 虚拟轮转法（Virtual RR）：主要基于时间片轮转法进行改进，解决在CPU调度中对于I/O密集型进程的不友好。其设置了一个辅助队列，对于I/O型进程执行完一个时间片之后，则进入辅助队列，CPU调度时总是先检查辅助队列是否为空，如果不为空总是优先调度辅助队列里的进程，直到为空，才调度就绪队列的进程。优先级调度算法（Priority Scheduling Algorithm——PSA）即给每个作业一个优先级，优先级越高越紧迫，应该先执行。通常：系统进程优先级高于用户进程；前台进程优先级高于后台进程；操作系统更偏好 I/O型进程。特点：实现简单，但不公平，可能导致优先级低的进程产生饥饿现象；可能产生优先级反转问题（基于优先级的抢占式算法），即一个低优先级进程持有一个高优先级进程所需要的资源，使得高优先级进程等待低优先级进程运行。 多级反馈队列调度算法（Multilevel Feedback）：设置多个就绪队列，并为各个队列赋予不同的优先级。第一个队列的优先级最高，依次递减优先级。对于各个队列进程执行时间片的大小也不同，优先级越高的队列，分配到的时间片越少。当第一级队列为空时，再第二级队列进行调度，依次类推，各级队列按照时间片轮转方式进行调度。当一个新进程创建后，首先把它放入第一队列的末尾。按照FCFS原则排队等待调度。当轮到该进程执行时，如它在该时间片完成，便可准备撤离系统，如果它在一个时间片结束时尚未完成，则调度程序便将该进程转入第二队列的末尾，再同样地按照FCFS原则等待调度执行。依次类推。特点：更偏好I/O型进程，对CPU型进程不太友好。各种调度算法总结比较：]]></content>
      <categories>
        <category>计算机</category>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>计算机</tag>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程]]></title>
    <url>%2F2019%2F06%2F03%2F%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[进程和线程的粗略对比线程，程序执行流的最小执行单位，是行程中的实际运作单位，经常容易和进程这个概念混淆。那么，线程和进程究竟有什么区别呢？首先，进程是一个动态的过程，是一个活动的实体。简单来说，一个应用程序的运行就可以被看做是一个进程，而线程，是运行中的实际的任务执行者。可以说，进程中包含了多个可以同时运行的线程。 引入多线程技术的动机在传统的操作系统中，进程是系统进行资源分配的基本单位，按进程为单位分给存放其映象所需要的虚地址空间、执行所需要的主存空间、完成任务需要的其他各类外围设备资源和文件。同时，进程也是处理器调度的基本单位，进程在任一时刻只有一个执行控制流，通常将这种结构的进程称单线程（结构）进程（single threaded process）。首先来考察一个文件服务器的例子，当它接受一个文件服务请求后，由于等待磁盘传输而经常被阻塞，假如不阻塞可继续接受新的文件服务请求并进行处理，则文件服务器的性能和效率便可以提高，由于处理这些请求时要共享一个磁盘缓冲区，程序和数据，要在同一个地址空间中操作。这一类应用非常多。 例如，航空售票系统需要处理多个购票和查询请求，这些信息都与同一个数据库相关；而操作系统在同时处理许多用户进程的查询请求时，都要去访问数据库所在的同一个磁盘。 对于上述这类基于同数据区的同时多请求应用，用单线程结构的进程难以达到这一目标，即使能解决问题代价也非常高，需要寻求新概念、提出新机制。随着并行技术、网络技术和软件设计技术的发展，给并发程序设计效率带来了一系列新的问题，主要表现在： 进程时空的开销大，频繁的进程调度将耗费大量处理器时间，要为每个进程分配存储空间限制了操作系统中进程的总数。 进程通信的代价大，每次通信均要涉及通信进程之间或通信进程与操作系统之间的信息传递。进程之间的并发性粒度较粗，并发度不高，过多的进程切换和通信延迟使得细粒度的并发得不偿失。 不适合并行计算和分布并行计算的要求，对于多处理器和分布式的计算环境来说，进程之间大量频繁的通信和切换，会大大降低并行度。 不适合客户/服务器计算的要求。对于 C/S 结构来说，那些需要频繁输入输出并同时大量计算的服务器进程（如数据库服务器、事务监督程序）很难体现效率。 这就迫切要求操作系统改进进程结构，提供新的机制，使得应用能够按照需求在同一进程中设计出多条控制流，多控制流之间可以并行执行，多控制流切换不需通过进程调度；多控制流之间还可以通过内存区直接通信，降低通信开销。这就是近年来流行的多线程（结构）进程（multiple threaded process） 。如果说操作系统中引入进程的目的是为了使多个程序能并发执行，以改善资源使用率和提高系统效率，那么，在操作系统中再引入线程，则是为了减少程序并发执行时所付出的时空开销，使得并发粒度更细、并发性更好。这里解决问题的基本思路是：把进程的两项功能－－“独立分配资源”与“被调度分派执行”分离开来，前一项任务仍由进程完成，它作为系统资源分配和保护的独立单位，不需要频繁地切换；后一项任务交给称作线程的实体来完成，它作为系统调度和分派的基本单位，会被频繁地调度和换，在这种指导思想下，产生了线程的概念。 进程和线程的本质对比进程是系统进行资源调度和分配的基本单位；线程是CPU调度的基本单位。 进程 = 资源 （包括寄存器值，PCB，内存映射表）+ TCB（栈结构）线程 = TCB（栈结构） 线程 的资源是共享的进程 间的资源是分隔独立的，内存映射表不同，占用物理内存地址是分隔的 线程 的切换只是切换PC，切换了TCB（栈结构）进程 的切换不仅要切换PC，还包括切换资源，即切换内存映射表 线程中的基本概念，线程的生命周期线程的生命周期，线程的生命周期可以利用以下的图解来更好的理解：先是用new Thread()的方法新建一个线程，在线程创建完成之后，线程就进入了就绪（Runnable）状态，此时创建出来的线程进入抢占CPU资源的状态，当线程抢到了CPU的执行权之后，线程就进入了运行状态（Running），当该线程的任务执行完成之后或者是非常态的调用的stop（）方法之后，线程就进入了死亡状态。而我们在图解中可以看出，线程还具有一个阻塞的过程，这是怎么回事呢？当面对以下几种情况的时候，容易造成线程阻塞，第一种，当线程主动调用了sleep（）方法时，线程会进入则阻塞状态，除此之外，当线程中主动调用了阻塞时的IO方法时，这个方法有一个返回参数，当参数返回之前，线程也会进入阻塞状态，还有一种情况，当线程进入正在等待某个通知时，会进入阻塞状态。那么，为什么会有阻塞状态出现呢？我们都知道,CPU的资源是十分宝贵的，所以，当线程正在进行某种不确定时长的任务时，Java就会收回CPU的执行权，从而合理应用CPU的资源。我们根据图可以看出，线程在阻塞过程结束之后，会重新进入就绪状态，重新抢夺CPU资源。这时候，我们可能会产生一个疑问，如何跳出阻塞过程呢?又以上几种可能造成线程阻塞的情况来看，都是存在一个时间限制的，当sleep()方法的睡眠时长过去后，线程就自动跳出了阻塞状态，第二种则是在返回了一个参数之后，在获取到了等待的通知时，就自动跳出了线程的阻塞过程 单线程和多线程单线程，顾名思义即是只有一条线程在执行任务，这种情况在我们日常的工作学习中很少遇到，所以我们只是简单做一下了解 多线程，创建多条线程同时执行任务，这种方式在我们的日常生活中比较常见。但是，在多线程的使用过程中，还有许多需要我们了解的概念。比如，在理解上并行和并发的区别，以及在实际应用的过程中多线程的安全问题，对此，我们需要进行详细的了解。 并发和并行：在我们看来，都是可以同时执行多种任务，那么，到底他们二者有什么区别呢？ 并发，从宏观方面来说，并行就是同时进行多种时间，实际上，这几种时间，并不是同时进行的，而是交替进行的，而由于CPU的运算速度非常的快，会造成我们的一种错觉，就是在同一时间内进行了多种事情 而并行，则是真正意义上的同时进行多种事情。这种只可以在多核CPU的基础下完成。 还有就是多线程的安全问题？为什么会造成多线程的安全问题呢？我们可以想象一下，如果多个线程同时执行一个任务，name意味着他们共享同一种资源，由于线程CPU的资源不一定可以被谁抢占到，这是，第一条线程先抢占到CPU资源，他刚刚进行了第一次操作，而此时第二条线程抢占到了CPU的资源，name，共享资源还来不及发生变化，就同时有两条数据使用了同一条资源，具体请参考多线程买票问题。这个问题我们应该如何解决那？ 由造成问题的原因我们可以看出，这个问题主要的矛盾在于，CPU的使用权抢占和资源的共享发生了冲突，解决时，我们只需要让一条线程战歌了CPU的资源时，阻止第二条线程同时抢占CPU的执行权，在代码中，我们只需要在方法中使用同步代码块即可。在这里，同步代码块不多进行赘述，可以自行了解。 线程池又以上介绍我们可以看出，在一个应用程序中，我们需要多次使用线程，也就意味着，我们需要多次创建并销毁线程。而创建并销毁线程的过程势必会消耗内存。而在Java中，内存资源是及其宝贵的，所以，我们就提出了线程池的概念。 线程池：Java中开辟出了一种管理线程的概念，这个概念叫做线程池，从概念以及应用场景中，我们可以看出，线程池的好处，就是可以方便的管理线程，也可以减少内存的消耗。 那么，我们应该如何创建一个线程池那?Java中已经提供了创建线程池的一个类：Executor 而我们创建时，一般使用它的子类：ThreadPoolExecutor. 1234567public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) 这是其中最重要的一个构造方法，这个方法决定了创建出来的线程池的各种属性，下面依靠一张图来更好的理解线程池和这几个参数：上图中，我们可以看出，线程池中的corePoolSize就是线程池中的核心线程数量，这几个核心线程，只是在没有用的时候，也不会被回收，maximumPoolSize就是线程池中可以容纳的最大线程的数量，而keepAliveTime，就是线程池中除了核心线程之外的其他的最长可以保留的时间，因为在线程池中，除了核心线程即使在无任务的情况下也不能被清除，其余的都是有存活时间的，意思就是非核心线程可以保留的最长的空闲时间，而util，就是计算这个时间的一个单位，workQueue，就是等待队列，任务可以储存在任务队列中等待被执行，执行的是FIFIO原则（先进先出）。threadFactory，就是创建线程的线程工厂，最后一个handler,是一种拒绝策略，我们可以在任务满了知乎，拒绝执行某些任务。 线程池的执行流程又是怎样的呢？有图我们可以看出，任务进来时，首先执行判断，判断核心线程是否处于空闲状态，如果不是，核心线程就先就执行任务，如果核心线程已满，则判断任务队列是否有地方存放该任务，若果有，就将任务保存在任务队列中，等待执行，如果满了，在判断最大可容纳的线程数，如果没有超出这个数量，就开创非核心线程执行任务，如果超出了，就调用handler实现拒绝策略。 handler的拒绝策略： 有四种：第一种AbortPolicy:不执行新任务，直接抛出异常，提示线程池已满第二种DisCardPolicy:不执行新任务，也不抛出异常第三种DisCardOldSetPolicy:将消息队列中的第一个任务替换为当前新进来的任务执行第四种CallerRunsPolicy:直接调用execute来执行当前任务 四种常见的线程池：CachedThreadPool:可缓存的线程池，该线程池中没有核心线程，非核心线程的数量为Integer.max_value，就是无限大，当有需要时创建线程来执行任务，没有需要时回收线程，适用于耗时少，任务量大的情况。 SecudleThreadPool:周期性执行任务的线程池，按照某种特定的计划执行线程中的任务，有核心线程，但也有非核心线程，非核心线程的大小也为无限大。适用于执行周期性的任务。 SingleThreadPool:只有一条线程来执行任务，适用于有顺序的任务的应用场景。 FixedThreadPool:定长的线程池，有核心线程，核心线程的即为最大的线程数量，没有非核心线程]]></content>
      <categories>
        <category>计算机</category>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>计算机</tag>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对进程的补充——内核]]></title>
    <url>%2F2019%2F06%2F03%2F%E5%AF%B9%E8%BF%9B%E7%A8%8B%E7%9A%84%E8%A1%A5%E5%85%85%E2%80%94%E2%80%94%E5%86%85%E6%A0%B8%2F</url>
    <content type="text"><![CDATA[内核是操作系统的内部核心程序，它向外部提供了对计算机设备的核心管理调用。我们将操作系统的代码分成两部分。内核所在的地址空间称作内核空间。而在内核以外的统称为外部管理程序，它们大部分是对外围设备的管理和界面操作。外部管理程序与用户进程所占据的地址空间称为外部空间。通常，一个程序会跨越两个空间。当执行到内核空间的一段代码时，我们称程序处于内核态，而当程序执行到外部空间代码时，我们称程序处于用户态。 Linux内核的核心功能如下图所示，Linux内核只是Linux操作系统一部分。对下，它管理系统的所有硬件设备；对上，它通过系统调用，向Library Routine（例如C库）或者其它应用程序提供接口。 因此，其核心功能就是：管理硬件设备，供应用程序使用。而现代计算机（无论是PC还是嵌入式系统）的标准组成，就是CPU、Memory（内存和外存）、输入输出设备、网络设备和其它的外围设备。所以为了管理这些设备，Linux内核提出了如下的架构。 Linux内核的整体架构和子系统划分 上图说明了Linux内核的整体架构。根据内核的核心功能，Linux内核提出了5个子系统，分别负责如下的功能： Process Scheduler也称作进程管理、进程调度。负责管理CPU资源，以便让各个进程可以以尽量公平的方式访问CPU。 Memory Manager内存管理。负责管理Memory（内存）资源，以便让各个进程可以安全地共享机器的内存资源。另外，内存管理会提供虚拟内存的机制，该机制可以让进程使用多于系统可用Memory的内存，不用的内存会通过文件系统保存在外部非易失存储器中，需要使用的时候，再取回到内存中。 VFS（Virtual File System）虚拟文件系统。Linux内核将不同功能的外部设备，例如Disk设备（硬盘、磁盘、NAND Flash、Nor Flash等）、输入输出设备、显示设备等等，抽象为可以通过统一的文件操作接口（open、close、read、write等）来访问。这就是Linux系统“一切皆是文件”的体现（其实Linux做的并不彻底，因为CPU、内存、网络等还不是文件，如果真的需要一切皆是文件，还得看贝尔实验室正在开发的”Plan 9”的）。 Network网络子系统。负责管理系统的网络设备，并实现多种多样的网络标准。 IPC（Inter-Process Communication）进程间通信。IPC不管理任何的硬件，它主要负责Linux系统中进程之间的通信。]]></content>
      <categories>
        <category>计算机</category>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>计算机</tag>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对进程的补充——PCB]]></title>
    <url>%2F2019%2F06%2F03%2F%E5%AF%B9%E8%BF%9B%E7%A8%8B%E7%9A%84%E8%A1%A5%E5%85%85%E2%80%94%E2%80%94PCB%2F</url>
    <content type="text"><![CDATA[进程控制块（PCB）在Linux中task_struct结构体即是PCB。PCB是进程的唯一标识，PCB由链表实现（为了动态插入和删除）。进程创建时，为该进程生成一个PCB；进程终止时，回收PCB。PCB包含信息：1、进程状态（state）；2、进程标识信息（uid、gid）；3、定时器（time）；4、用户可见寄存器、控制状态寄存器、栈指针等（tss） 每个进程都有一个非负的唯一进程ID（PID）。虽然是唯一的，但是PID可以重用，当一个进程终止后，其他进程就可以使用它的PID了。PID为0的进程为调度进程，该进程是内核的一部分，也称为系统进程；PID为1的进程为init进程，它是一个普通的用户进程，但是以超级用户特权运行；PID为2的进程是页守护进程，负责支持虚拟存储系统的分页操作。除了PID，每个进程还有一些其他的标识符： 每个进程的task_struct和系统空间堆栈(系统表格区)存放位置如下：两个连续的物理页【《Linux内核源代码情景分析》271页】系统堆栈空间不能动态扩展，在设计内核、驱动程序时要避免函数嵌套太深，同时不宜使用太大太多的局部变量，因为局部变量都是存在堆栈中的。 进程的创建新进程的创建，首先在内存中为新进程创建一个task_struct结构，然后将父进程的task_struct内容复制其中，再修改部分数据。分配新的内核堆栈、新的PID、再将task_struct这个node添加到链表中。所谓创建，实际上是“复制”。 子进程刚开始，内核并没有为它分配物理内存，而是以只读的方式共享父进程内存，只有当子进程写时，才复制。即“copy-on-write”。fork都是由do_fork实现的，do_fork的简化流程如下图：fork函数fork函数时调用一次，返回两次。在父进程和子进程中各调用一次。子进程中返回值为0，父进程中返回值为子进程的PID。程序员可以根据返回值的不同让父进程和子进程执行不同的代码。一个形象的过程：运行这样一段演示程序： 123456789101112131415161718192021222324252627#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;int main()&#123; pid_t pid; char *message; int n = 0; pid = fork(); while(1)&#123; if(pid &lt; 0)&#123; perror("fork failed\n"); exit(1); &#125; else if(pid == 0)&#123; n--; printf("child's n is:%d\n",n); &#125; else&#123; n++; printf("parent's n is:%d\n",n); &#125; sleep(1); &#125; exit(0);&#125;]]></content>
      <categories>
        <category>计算机</category>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>计算机</tag>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程]]></title>
    <url>%2F2019%2F06%2F02%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F3%2F</url>
    <content type="text"><![CDATA[计算机中，CPU是最宝贵的资源，为了提高CPU的利用率，引入了多道程序设计的概念。当内存中多个程序存在时，如果不对人们熟悉的“程序”的概念加以扩充，就无法刻画多个程序共同运行时系统呈现出的特征。 进程的引入多道程序系统中，程序具有：并行、制约以及动态的特征。 程序概念难以反映系统中的情况： 程序是一个静态的概念程序是完成某个功能的指令集和。系统实际上是出于不断变化的状态中，程序不能反映这种动态性。 程序概念不能反映系统中的并行特性例如：两个C语言源程序由一个编译程序完成编译，若用程序概念理解，内存中只有一个编译程序运行（两个源程序看作编译程序的输入数据），但是这样无法说明白内存中运行着两个任务。程序的概念不能表示这种并行情况，反映不了他们活动的规律和状态变化。就像不能用菜谱（程序）代替炒菜（程序执行的过程）一样（这句话我稍微修改了一下，感觉应该是这样表诉才对） 进程的定义进程：一个具有一定独立功能的程序关于某个数据集合的一次运行活动，是系统进行资源分配和调度运行的基本单位 进程与程序的差别进程是一个动态的概念进程是程序的一次执行过程，是动态概念程序是一组有序的指令集和，是静态概念 不同的进程可以执行同一个程序区分进程的条件：所执行的程序和数据集合。两个进程即使执行在相同的程序上，只要他们运行在不同的数据集合上，他们也是两个进程。例如：多个用户同时调用同一个编译程序编译他们编写的C语言源程序，由于编译程序运行在不同的数据集合（不同的C语言源程序）上，于是产生了一个个不同的进程 每个进程都有自己的生命周期当操作系统要完成某个任务时，它会创建一个进程。当进程完成任务之后，系统就会撤销这个进程，收回它所占用的资源。从创建到撤销的时间段就是进程的生命期 进程之间存在并发性在一个系统中，同时会存在多个进程。他们轮流占用CPU和各种资源 进程间会相互制约进程是系统中资源分配和运行调度的单位，在对资源的共享和竞争中，必然相互制约，影响各自向前推进的速度 进程可以创建子进程，程序不能创建子程序从结构上讲每个进程都由程序、数据和一个进程控制块（Process Control Block, PCB）组成 进程的重要特征动态特征：进程对应于程序的运行，动态产生、消亡，在其生命周期中进程也是动态的 并发特征：任何进程都可以同其他进程一起向前推进 独立特征：进程是相对完整的调度单位，可以获得CPU，参与并发执行 交往特征：一个进程在执行过程中可与其他进程产生直接或间接关系 异步特征：每个进程都以相对独立、不可预知的速度向前推进 结构特征：每个进程都有一个PCB作为他的数据结构 进程最基本的特征是并发和共享特征 进程的状态与转换进程的三种基本状态a.运行状态：获得CPU的进程处于此状态，对应的程序在CPU上运行着b.阻塞状态：为了等待某个外部事件的发生（如等待I/O操作的完成，等待另一个进程发来消息），暂时无法运行。也成为等待状态c.就绪状态：具备了一切运行需要的条件，由于其他进程占用CPU而暂时无法运行 进程状态转换a.运行状态 ===&gt; 阻塞状态：例如正在运行的进程提出I/O请求，由运行状态转化为阻塞状态b.阻塞状态 ===&gt; 就绪状态：例如I/O操作完成之后，由阻塞状态转化为就绪状态c.就绪状态 ===&gt; 运行状态：例如就绪状态的进程被进程调度程序选中，分配到CPU中运行，由就绪状态转化为运行状态d.运行状态 ===&gt; 就绪状态：处于运行状态的进程的时间片用完，不得不让出uCPU，由运行状态转化为就绪状态 进程的类型a.系统进程：操作系统用来管理资源的进程，当系统进程处于运行态时，CPU处于管态，系统之间的关系由操作系统负责b.用户进程：操作系统可以独立执行的的用户程序段，当用户进程处于运行态时，CPU处于目态，用户进程之间的关系由用户负责 进程控制块进程的三个组成部分a. 程序b. 数据c. 进程控制块（PCB）：为了管理和控制进程，系统在创建每个进程时，都为其开辟一个专用的存储区，用以记录它在系统中的动态特性。系统根据存储区的信息对进程实施控制管理。进程任务完成后，系统收回该存储区，进程随之消亡，这一存储区就是进程控制块 PCB随着进程的创建而建立，撤销而消亡。系统根据PCB感知一个进程的存在，PCB是进程存在的唯一物理标识（这一点可以类比作业控制块JCB） 进程控制块的内容PCB在不同的语言中，可能用不同的数据结构表示。为了系统管理和控制进程方便，系统常常将所有进程的PCB存放在内存中系统表格区(系统空间堆栈)（这是什么区？不懂，待我仔细查查），并按照进程内部标号由小到大顺序存放。 整个系统中各进程的的PCB集合可用数组表示。这时进程内部标号可以与数组元素下标联系。 各系统预留的PCB空间往往是固定的，如UNIX系统中规定进程数量不超过50个（这一点我有点怀疑） 操作系统不同，PCB的格式、大小及内容也不尽相同。一般的，应该包含如下四个信息 a. 标识信息：进程名（uid、gid）b. 说明信息：进程状态（state）、程序存放位置c. 现场信息：通用寄存器内存、控制寄存器内存、断点地址d. 管理信息：进程优先数、队列指针 进程控制块的组织系统中，有着许多不同状态的进程，处于阻塞状态的进程阻塞原因各不相同，为了便于调度和管理，常将进程控制块PCB用适当的方法组织起来 线性结构把所有不同状态的进程的PCB组织在一个表格中。最简单，适用于进程数目不多的操作系统，如UNIX系统，缺点是调用时，往往需要查询整个PCB表，时间复杂度略高 索引结构分别把具有不同状态的进程PCB组织在同一个表中，于是有就绪进程表、运行进程表（多机系统中，还有现在的多核系统应该也有吧）以及各种等待事件的阻塞进程表 系统中的一些固定单元分别指出各表的起始地址 链式结构采用队列形式时，每个进程的PCB中要增加一个链指针表项，指向队列的下一个PCB起始地址。为了对这些队列进行管理，操作系统要做三件事： a. 把处于同一状态的进程的PCB通过各自队列的指针链接在一起，形成队列b. 为每一个队列设立一个对头指针，总是指向队首的PCBc. 排在队尾的PCB的队列指针项内容应该是“-1”或者一个特殊符号，表示这是队尾PCB 在单CPU系统中，任何时刻都只有一个处于运行态的进程 所有处于阻塞队列中的PCB应该根据产生阻塞的原因今进行排队，每一个都称为阻塞队列，比如等待磁盘I/O的阻塞队列，等待打印机输出的阻塞队列 进程控制原语要对进程进行控制，系统中必须设置一个机构，它具有创建进程、撤销进程、进程通信和资源管理等功能，这样的结构称为操作系统的内核（kernel）内核本身不是一个进程，而是硬件的首次延伸，它是加在硬件上的第一层软件。内核是通过执行各种原语操作来完成各种控制和管理功能的原语（primitive）是机器指令的延伸，用若干条机器指令构成，用以完成特定功能的一段程序。为保证操作的正确性，原语在执行期间是不可分割的（这点可以类比数据库中的事务）用于进程控制的原语有：创建进程原语、撤销进程原语、阻塞进程原语、唤醒进程原语、调度进程原语、改变优先级原语等 创建进程原语一个进程如果需要时，可以创建一个新的进程。被建立的进程称为子进程，建立者进程称为父进程所有的进程都只能通过父进程建立，不能自生自灭。 创建进程原语供进程调用，用以建立子进程。该原语的主要工作：为被建立的进程简历一个进程控制块，填入相应的初始值。主要操作过程是先向系统的PCB空间申请分配一个空闲的PCB，然后根据父进程所提供的参数，将子进程的PCB表目初始化，最后返回一个子进程内部名。 撤销进程原语由父进程撤销子进程的PCB，注意，这里会撤销一个以该子进程为根的进程子树，并回收占用的全部资源 阻塞原语在阻塞原语的作用下，进程由运行状态转化为阻塞状态 唤醒原语在唤醒原语的作用下，进程由阻塞状态转化为就绪状态 改变进程优先级原语进程的优先级是表示进程的重要性以及运行的优先性，拱进程调度程序调度进程运行时使用为了防止一些进程因优先级较低，而长期得不到运行，许多系统采用动态优先级，进程的优先级按照一些原则变化通常，进程优先级和以下因素有关系： a. 作业开始时的静态优先数：作业的优先数取决于作业的重要程度、用户为作业运行时所付出的价格和费用大小、作业的类型等因素b. 进程的类型：一般系统进程的优先数大于用户进程的优先数；I/O型进程的优先数大于CPU型进程的优先数。这些都是为了充分发挥系统I/O设备的效能c. 进程所使用的资源量：使用CPU的时间越多，优先级越低。对其他资源使用的情况也类似的考虑d. 进程在系统中的等待时间：等待时间越长，进程优先级越高 各系统处于不同的考虑，有不同的优先数计算公式。这些公式主要来自于时间经验]]></content>
      <categories>
        <category>计算机</category>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>计算机</tag>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作系统后期发展]]></title>
    <url>%2F2019%2F05%2F29%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F2%2F</url>
    <content type="text"><![CDATA[通用操作系统 操作系统的三种基本类型：多道批处理系统、分时系统、实时系统。 通用操作系统：具有多种类型操作特征的操作系统。可以同时兼有多道批处理、分时、实时处理的功能，或其中两种以上的功能。 例如：实时处理+批处理=实时批处理系统。首先保证优先处理实时任务，插空进行批处理作业。常把实时任务称为前台作业，批作业称为后台作业。再如：分时处理+批处理=分时批处理系统。即：时间要求不强的作业放入“后台”（批处理）处理，需频繁交互的作业在“前台”（分时）处理，处理机优先运行“前台”作业。 从上世纪60年代中期，国际上开始研制一些大型的通用操作系统。这些系统试图达到功能齐全、可适应各种应用范围和操作方式变化多端的环境的目标。但是，这些系统过于复杂和庞大，不仅付出了巨大的代价，且在解决其可靠性、可维护性和可理解性方面都遇到很大的困难。相比之下，UNIX操作系统却是一个例外。这是一个通用的多用户分时交互型的操作系统。它首先建立的是一个精干的核心，而其功能却足以与许多大型的操作系统相媲美，在核心层以外，可以支持庞大的软件系统。它很快得到应用和推广，并不断完善，对现代操作系统有着重大的影响。至此，操作系统的基本概念、功能、基本结构和组成都已形成并渐趋完善。 操作系统的进一步发展进入20世纪80年代，大规模集成电路工艺技术的飞跃发展，微处理机的出现和发展，掀起了计算机大发展大普及的浪潮。一方面迎来了个人计算机的时代，同时又向计算机网络、分布式处理、巨型计算机和智能化方向发展。于是，操作系统有了进一步的发展，如：个人计算机操作系统、网络操作系统、分布式操作系统等。 个人计算机操作系统个人计算机上的操作系统是联机交互的单用户操作系统，它提供的联机交互功能与通用分时系统提供的功能很相似。由于是个人专用，因此一些功能会简单得多。然而，由于个人计算机的应用普及，对于提供更方便友好的用户接口和丰富功能的文件系统的要求会愈来愈迫切。 网络操作系统计算机网络：通过通信设施，将地理上分散的、具有自治功能的多个计算机系统互连起来，实现信息交换、资源共享、互操作和协作处理的系统。网络操作系统：在原来各自计算机操作系统上，按照网络体系结构的各个协议标准增加网络管理模块，其中包括：通信、资源共享、系统安全和各种网络应用服务。 分布式操作系统表面上看，分布式系统与计算机网络系统没有多大区别。分布式操作系统也是通过通信网络，将地理上分散的具有自治功能的数据处理系统或计算机系统互连起来，实现信息交换和资源共享，协作完成任务。——硬件连接相同。但有如下一些明显的区别：（1）分布式系统要求一个统一的操作系统，实现系统操作的统一性。（2）分布式操作系统管理分布式系统中的所有资源，它负责全系统的资源分配和调度、任务划分、信息传输和控制协调工作，并为用户提供一个统一的界面。（3）用户通过这一界面，实现所需要的操作和使用系统资源，至于操作定在哪一台计算机上执行，或使用哪台计算机的资源，则是操作系统完成的，用户不必知道，此谓：系统的透明性。（4）分布式系统更强调分布式计算和处理，因此对于多机合作和系统重构、坚强性和容错能力有更高的要求，希望系统有：更短的响应时间、高吞吐量和高可靠性。 操作系统的作用现代的计算机系统主要是由一个或者多个处理器，主存，硬盘，键盘，鼠标，显示器，打印机，网络接口及其他输入输出设备组成。一般而言，现代计算机系统是一个复杂的系统。其一：如果每位应用程序员都必须掌握该系统所有的细节，那就不可能再编写代码了（严重影响了程序员的开发效率：全部掌握这些细节可能需要一万年….）其二：并且管理这些部件并加以优化使用，是一件极富挑战性的工作，于是，计算安装了一层软件（系统软件），称为操作系统。它的任务就是为用户程序提供一个更好、更简单、更清晰的计算机模型，并管理刚才提到的所有设备。 总结程序员无法把所有的硬件操作细节都了解到，管理这些硬件并且加以优化使用是非常繁琐的工作，这个繁琐的工作就是操作系统来干的，有了他，程序员就从这些繁琐的工作中解脱了出来，只需要考虑自己的应用软件的编写就可以了，应用软件直接使用操作系统提供的功能来间接使用硬件。 精简的说操作系统就是一个协调、管理和控制计算机硬件资源和软件资源的控制程序。操作系统所处的位置如图 细说的说操作系统应该分成两部分功能：一：隐藏了丑陋的硬件调用接口，为应用程序员提供调用硬件资源的更好，更简单，更清晰的模型（系统调用接口）。应用程序员有了这些接口后，就不用再考虑操作硬件的细节，专心开发自己的应用程序即可。例如：操作系统提供了文件这个抽象概念，对文件的操作就是对磁盘的操作，有了文件我们无需再去考虑关于磁盘的读写控制（比如控制磁盘转动，移动磁头读写数据等细节），二：将应用程序对硬件资源的竞态请求变得有序化例如：很多应用软件其实是共享一套计算机硬件，比方说有可能有三个应用程序同时需要申请打印机来输出内容，那么a程序竞争到了打印机资源就打印，然后可能是b竞争到打印机资源，也可能是c，这就导致了无序，打印机可能打印一段a的内容然后又去打印c…,操作系统的一个功能就是将这种无序变得有序。]]></content>
      <categories>
        <category>计算机</category>
        <category>历史</category>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>计算机</tag>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作系统初期发展]]></title>
    <url>%2F2019%2F05%2F21%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F1%2F</url>
    <content type="text"><![CDATA[无操作系统的计算机系统:人工操作方式：用户采用人工操作方式直接使用计算机硬件系统，将已经穿孔的纸带装入纸带输入机，在启动它将程序和数据输入计算机，然后计算机运行。 在人工方式下，其最大的缺点：用户独占全机，CPU等待人工操作（计算机资源利用率很低）；直到出现脱机输入/输出方式。 由于程序和数据都在外围机的控制下完成，或者说，他们在脱离主机的情况下进行的，故此称为脱机输入/输出 ,反之，在主机的直接控制下进行的输入/输出的方式称为联机输入/输出方式。 优点：减少了CPU的空闲时间，提高了I/O速度。 脱机处理时，外部设备上的数据需要一个相当长的等待时间后才被进行处理。当外部设备上有数据输入时，主机并不予处理，只是将外部设备的数据存放到缓冲区中。一旦缓冲区满了，或是等待的时间到了，主机才进行加工处理。对输出的操作也是这样，一旦计算机要把处理结果输出，它只是把输出结果送入缓冲区中，然后向外部设备慢慢地进行输出，而主机又去进行其它的加工处理，当缓冲区中的数据全部输出完毕，主机再把下一批的数据存入缓冲区中。 手工操作(穿孔卡片)1946年第一台计算机诞生–20世纪50年代中期，计算机工作还在采用手工操作方式。此时还没有操作系统的概念。程序员将对应于程序和数据的已穿孔的纸带（或卡片）装入输入机，然后启动输入机把程序和数据输入计算机内存，接着通过控制台开关启动程序针对数据运行；计算完毕，打印机输出计算结果；用户取走结果并卸下纸带（或卡片）后，才让下一个用户上机。 手工操作方式两个特点：1.用户独占全机。不会出现因资源已被其他用户占用而等待的现象，但资源的利用率低。2.CPU 等待手工操作。CPU的利用不充分。 20世纪50年代后期，出现人机矛盾：手工操作的慢速度和计算机的高速度之间形成了尖锐矛盾，手工操作方式已严重损害了系统资源的利用率（使资源利用率降为百分之几，甚至更低），不能容忍。唯一的解决办法：只有摆脱人的手工操作，实现作业的自动过渡。这样就出现了成批处理。 批处理系统(磁带存储)诞生时间：20世纪50年代。 实时操作系统的起源为了提高单一操作员单一控制终端的操作系统SOSC的效率，人们提出了批处理操作系统。SOSC效率低下是因为计算机总是在等待人的下一步动作，而人的动作总是很慢。因此，人们觉得，如果去掉等待人的时间，即让所有的人先想好自己要运行的命令，列成一个清单，打印在纸带上，然后交给一个工作人员来一批一批的处理，效率不就提高了吗？这样就形成了批处理操作系统。 批处理系统的定义批处理系统就是成批处理一些程序的系统。批处理分为联机批处理和脱机批处理两种。 联机批处理在联机批处理中，编制了一个常驻内存的监督程序，用来控制用户作业的运行。其处理过程为：用户将所需解决的问题组成作业，交给操作员，操作员有选择地把若干作业合成一批，并把一批作业装到输入设备上，然后由监督程序控制送到辅存，再从辅村中将一个一个作业调入内存运行，直到全部作业处理完毕。 脱机批处理脱机批处理系统由主机和卫星机组成。卫星机又称外围计算机，它不与主机直接连接，只与外部设备打交道。作业通过卫星机输入到磁带上，当主机需要输入作业时，就把输入带从卫星机的磁带机上取下，并装入到主机的磁带机上。于是，主机可以连续的处理由输入带输入的许多用户作业，并把这些作业的运行结果不断地输出到输出带上。最后，多个用户作业的输出结果再通过卫星机连接的打印机打印出来。 脱机批处理产生的目的：缓解主机与外设的矛盾提高CPU的利用率。 多道批处理系统诞生时间：20世纪60年代 多道程序系统多道程序系统是控制多道程序同时运行的程序系统，由它决定在某一个时刻运行哪一个作业，或者说，是在计算机内存中同时存放几道互相独立的程序，使他们在管理程序控制之下，相互穿插地运行，即使多道程序在系统内并行工作。 主要特征：1.多道，即计算机内存中同时存放几道相互独立的程序。2.宏观上并行，同时进入系统的几道程序都处于运行过程中，即他们先后开始了各自的运行，但都未运行完毕。3.微观上串行，内存中的多道程序轮流地或分时地占有CPU，交替执行 多道程序系统的出现，标志着操作系统渐趋成熟的阶段，先后出现了作业调度管理、处理机管理、存储器管理、外部设备管理、文件系统管理等功能。由于多个程序同时在计算机中运行，开始有了空间隔离的概念，只有内存空间的隔离，才能让数据更加安全、稳定。出了空间隔离之外，多道技术还第一次体现了时空复用的特点，遇到IO操作就切换程序，使得cpu的利用率提高了，计算机的工作效率也随之提高。 多道批处理系统多道批处理系统有两个含义：一是多道，二是批处理。多道是在计算机内存中同时存放多个作业，它们在操作系统的控制下并发执行，而且在外存中还存放有大量的作业，并组成一个后备作业队列，系统按一定的调度原则每次从后备作业队列中选取一个或多个作业调入内存运行。 分时系统 分时系统的起源背景：在多道批处理系统的时代，人们主要提高对系统资源的利用率和系统的吞吐量。但是由于时代的发展，人们又提出了另一个问题，在人们将制作在卡片上的程序交由计算机执行时，用户无法即时获得程序运行的结果。这一问题很有可能导致很严重的后果发生。基于这个问题，人们考虑能否让人回到计算机前来，每个人即时管理自己的程序，但又由于20世纪60年代计算机还十分昂贵，所以一台计算机要同时供多个用户共享使用，每个用户在共享一台计算机时都希望能像独占时一样，不仅可以随时与计算机进行交互，而且还不会感觉到其他用户的存在。于是分时系统就在这样情况下诞生。 分时系统的介绍分时系统是允许多个联机用户同时使用一台计算机进行处理的系统。系统将CPU在时间上分割成很小的时间段，每个时间段称为一个时间片。每个联机用户通过终端以交互方式控制程序的运行，系统把CPU时间轮流分配给个联机作业，每个作业只运行极短的时间片，从而使每个用户都有一种“独占计算机”的感觉。 分时系统实现的关键问题人—机交互问题1.及时接收2.及时处理 共享主机问题分时系统的主要目标为了方便用户使用计算机系统，并在尽可能的情况下，提高系统资源的利用率。 分时系统的主要特征1.多路性是指系统允许将多台终端同时连接到主机上，并按分时原则为每个用户服务。多路性允许多个用户共享一台主机，显著提高资源利用率，降低使用费用，促进计算机更广泛的应用。2.独立性是指系统提供了这样的用机环境，即每个用户在各自的终端上进行操作，彼此之间互不干扰，给用户的感觉就像是一个人在使用主机。3.交互性是指用户可通过终端与系统进行广泛的人人机对话。其广泛性表现在：用户可以请求系统提供多方面的服务，如进行文件编辑和数据处理，访问系统中的文件和数据库，请求提供打印服务等。4.及时性是指用户的请求能在很短的时间内获得响应。这一时间间隔是根据人们所能接受的等待时间确定的，通常仅为1~3秒钟。 分时系统的优点1.自然操作方式该系统使用户能在较短的时间内采用交互式会话工作方式，及时输入、调度、修改和运行自己的程序，因而加快了解题周期。2.扩大了应用范围无论是本地用户，还是运地用户，只要与计算机连在一台终端设备，就可以随时随地使用计算机。3.便于共享和交换信息远近终端用户均通过系统中的文件系统彼此交流信息和共享各种文件。4.经济实惠用户只须有系统配备的终端，即可完成各种处理任务，可共享大型的具有丰富资源的计算机系统。 分时系统实例解析若选择时间片为100ms，系统中有20个用户分享CPU，并忽略用户程序间的切换时间开销，则每个用户的平均响应时间为：100ms*20=2秒。在假设CPU运行速度为200万次/秒，则对每个用户程序来说，等价的CPU速度为：200/20=10万次/秒。 注意：分时系统的分时间片工作，在没有遇到IO操作的时候就用完了自己的时间片被切走了，这样的切换工作其实并没有提高cpu的效率，反而使得计算机的效率降低了。但是我们牺牲了一点效率，却实现了多个程序共同执行的效果，这样你就可以在计算机上一边听音乐一边聊qq了。 实时操作系统 实时操作系统的起源1980年，加拿大两个大学生Gordon Bell和Dan Dodge，在学习操作系统设计课程期间，萌发了设计“实时操作系统（RTOS）”的念头，并且动手干了起来，最终做出了一个实时操作系统的微内核。 实时操作系统的定义实时操作系统是保证在一定时间限制内完成特定功能的操作系统。实时操作系统有软实时系统和硬实时系统之分。软实时系统在规定时间得不到响应所产生的后果是可以承受的，如流水装配线。即使装配线瘫痪，也只是损失了资金；而硬实时系统在得不到实时响应后则可能产生不能承受的灾难，如导弹防卫系统。如果反应迟钝，结果就可能是严重损失。 实时操作系统的分类计算机应用到实时控制中，配置实时操作系统，就可组成各种各样的实时系统。目前，在计算机应用中，过程控制和信息处理都有一定的实时要求，据此，把实时系统分为实时过程控制系统和实时信息处理系统两大类。 实时过程控制它又可分为两类：一类是以计算机位控制中枢的生产过程自动化系统，如冶炼、发电、炼油、化工、机械加工等的自动控制。在这类系统中，要求计算机及时采集和处理现场信息，控制有关的执行装置，使得某些参数，如温度、压力、流量、液位等按一定规律变化，从而达到实现生产过程自动化的目的。另一类是飞行物体的自动控制，如飞机、导弹、人造卫星的制导等。这类系统要求反应速度快，可靠性高。通常要求系统的响应时间在毫秒甚至微秒级内。 实时信息处理它通常配有大型文件系统或数据，事先存有经过合理组织的大量数据，它能及时响应来自终端用户的服务请求，如进行信息的检索、存储、修改、更新、加工、删除、传递等，并能在短时间内对用户作出正确的回答。如情报检索、机票预定、银行业务、电话交换等都属此类系统。这类系统除要求响应时间及时外，并要求有较高的可靠性、安全性和保密措施等。 实时操作系统的特点 对外部进入系统的信号或信息应能做到实时响应。 实时系统较一般的通用系统有规律，有许多操作具有一定的可预计性。 实时系统的终端一般作为执行和询问使用，不具有分时系统那样有较强的会话能力。 实时系统对可靠性和安全性要求较高，常采用双工工作方式。 实时系统的代表——VxWorksVxWorks 操作系统是美国WindRiver公司于1983年设计开发的一种嵌入式实时操作系统，是嵌入式开发环境的关键组成部分。良好的持续发展能力、高性能的内核以及友好的用户开发环境，在嵌入式实时操作系统领域占据一席之地。它以其良好的可靠性和卓越的实时性被广泛地应用在通信、军事、航空、航天等高精尖技术及实时性要求极高的领域中，如卫星通讯、军事演习、弹道制导、飞机导航等。在美国的 F-16、FA-18战斗机、B-2 隐形轰炸机和爱国者导弹上，甚至连1997年4月在火星表面登陆的火星探测器、2008年5月登陆的凤凰号，和2012年8月登陆的好奇号也都使用到了VxWorks上。]]></content>
      <categories>
        <category>计算机</category>
        <category>历史</category>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>计算机</tag>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作系统年表]]></title>
    <url>%2F2019%2F05%2F20%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F0%2F</url>
    <content type="text"><![CDATA[1950年代1956年 GM-NAA I/O 1959年 SHARE Operating System 1960年代1960年 IBSYS 1961年 CTSSMCP (Burroughs Large Systems) 1962年 GCOS 1964年 EXEC 8OS/360（宣称）TOPS-10 1965年 Multics（宣称）OS/360（上市）Tape Operating System（TOS） 1966年 DOS/360（IBM）MS/8 1967年 ACP（IBM）CP/CMSITSWAITS 1969年 TENEXUnix 1970年代1970年 DOS/BATCH 11（PDP-11） 1971年 OS/8 1972年 MFT (operating system)MVTRDOSSVSVM/CMS 1973年 Alto OSRSX-11DRT-11VME 1974年 MVS（MVS/XA） 1975年 BS2000 1976年 CP/MTOPS-20 1978年 Apple DOS 3.1（苹果公司第一个操作系统）TripOSVMSLisp Machine（CADR） 1979年 POSNLTSS 1980年代1980年 OS-9QDOSSOSXDE (Tajo)Xenix 1981年 MS-DOS 1982年 Commodore DOSSunOS(1.0)Ultrix 1983年 Lisa OSCoherentNovell NetWareProDOS 1984年 Macintosh OS（系统 1.0）MSX-DOSQNXUniCOS 1985年 AmigaOSAtari TOSMIPS OSOberon operating systemMicrosoft Windows 1.0（Windows第一版） 1986年 AIXGS-OSHP-UX 1987年 ArthurIRIX（SGI推出的第一个版本号是3.0）MinixOS/2（1.0）Microsoft Windows 2.0 1988年 A/UX（苹果电脑）LynxOSMVS/ESAOS/400 1989年 NeXTSTEP（1.0）RISC OSSCO Unix（第三版） 1990年代1990年 Amiga OS2.0BeOS（v1）OSF/1Microsoft Windows 3.0 1991年 Linux 1992年 386BSD0.1Amiga OS3.0Solaris2.0 （SunOS 4.x的继承者，以SVR4为基础，而非BSD）Microsoft Windows 3.1 1993年 九号项目（第一版）FreeBSDNetBSDMicrosoft Windows NT 3.1（第一版NT） 1995年 Digital UNIX（akaTru64）OpenBSDOS/390Microsoft Windows 95 1996年 Windows NT 4.0Linux2.0 1997年 InfernoMac OS 7.6（第一版官方正式命名为Mac OS）SkyOS 1998年 Solaris7 （第一款64位Solaris版本，是2.7舍弃主版本号的称谓）Microsoft Windows 98 1999年 AROSMac OS 8Microsoft Windows 98 Second Edition 2000年代2000年 AtheOSMac OS 9MorphOSWindows 2000Microsoft Windows Me 2001年 Amiga OS4.0 （2001年5月）Mac OS X 10.1Windows XPz/OS 2002年 Windows XP 64-bit EditionSyllableMac OS X 10.2 2003年 Windows Server 2003（2003年3月28日）Windows XP 64-bit Edition- 以Microsoft Windows Server 2003为基础，同一天发布。Mac OS X 10.3 2004年 Windows XP Media Center Edition 2005年 Windows XP Professional x64 EditionMac OS X 10.4 2006年 MicrosoftWindows Vista 2007年 Mac OS X 10.5 2009年 Mac OS X 10.6MicrosoftWindows 7]]></content>
      <categories>
        <category>计算机</category>
        <category>历史</category>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>计算机</tag>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机历史——软件的发展]]></title>
    <url>%2F2019%2F05%2F19%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8E%86%E5%8F%B20%2F</url>
    <content type="text"><![CDATA[软件从未变得如此重要。当年赴摩尔学院听计算机讲座的先驱们万万没有想到，他们所学的知识竟然催生了一个新的行业，一个由计算机指令支撑起来的虚拟行业。软件分为多个层次，涉及到微代码、机器代码、汇编语言、高级语言的转换。数据库、软件开发过程，乃至整个企业结构都与之息息相关。计算机无所不在，它们的思想统领着整个世界。软件的重要性已变得无以复加，无怪乎世界上财力最雄厚的富豪当中，有好几个人是靠经营软件公司而积累了大笔的财富。 “优化软件就相当于优化一切。” 数据库随着编程语言的功能日渐强大，计算机的速度日益加快，容量日益扩大，可靠性日益增强。很快，计算机科学家就意识到，除了计算方程式的结果，计算机还可以在更广阔的领域大有可为。它们可以快速存储和处理大量的数据。如果只是单纯存储一系列数字，那倒不是什么难事，但是，如果存储的是病历记录或者财会记录，那么情况就会复杂许多。这就要求计算机必须能在短时间内迅速找到特定的信息条目——比如某一份病例中记述的病因，或者某一天产生的支付记录。怎样编排数据才能帮助我们轻松地找到所需信息呢？这些都不是什么新的问题——自从第一部词典问世以来，人们就面临着快速查找词条的问题。其解决办法，就是按照字母表的顺序编排词语。以某个单词为参照，词典中的其他词语中要么在它之前，要么在它之后。因此，我们在查找单词时，可以翻开词典的任何一页，然后根据上面的词语决定应该往前翻还是往后翻，直到将我们想查的单词查找出来。 第一代第一代数据库都是按照非常类似的原则编排的，它们于20世纪60年代问世，被称为导航式数据库管理系统（navigational database management system）。数据库第一次打开时，就会自动出现一个指针，指向第一条存储记录（好比翻开词典正文的第一页，就会看到第一个词条是以a开头的单词）。这些存储记录按照链表或网状结构编排，每一条记录当中都存有一个指针，指向相邻的记录。为了检索信息，计算机需要访问每一条记录，并通过其中的链接跳转到下一个记录，以此类推，直到找到用户所需的信息。如果用户想从一个庞大的数据库中检索出所有心脏病患者的病例，那么计算机就需要把每一条记录都访问一遍，才能将心脏病患者的病例筛选出来。这显然是一个非常缓慢而低效的信息检索方式，而且在那个年代，计算机和存储设备的运转速度都慢得可怜，要执行这样的检索是难以想象的。或许这种事情在今天的人看来有些匪夷所思，不过第一代数据库的确是不能用来查找信息。随着计算机和存储器的性能日渐提升，导航式数据库的局限性也愈发凸显。20世纪70年代，IBM的一名英国研究人员埃德加·弗兰克·科德（Edgar Frank “Ted” Codd）认为，数据库的模型亟待改进。科德与开发硬盘存储系统的IBM员工一道，设计了一个新的数据库模型，科德称之为关系模型。这种数据库结构比较智能化。数据的编排方式并不是像词典那样列出一长串条目，而是采用一系列数据库表，不同的数据库表通过关系键联系起来。为了便于大家理解关系数据库，我们用词典来打个比方。关系数据库就好比一系列分类词典，比如动词词典、名词词典、副词词典等等。如果你想查找某个单词——比如动词“bow”，那就只需要翻阅动词词典。如果你想找“bow”的名词词义，那就只需要翻阅名词词典。这比从头到尾地翻阅一本超厚的大词典要高效多了。关系数据库也是同样的道理，它采用关系键，将不同的记录联系起来。比方说，一个员工信息数据库可能会以员工编号作为关系键。知道了工号，你就可以在各个数据库表（好比各个分类词典）中分别查询员工的工资信息、家庭住址、计算机登录情况等等。每个数据库表中的值都可以作为关系键，用于在其他数据库表中查找相应的信息。也就是说，员工的职位描述也可作为关系键，用来查询员工是否具备参与机密工作的资格。 SEQUEL的出现20世纪70年代末出现了一种特殊的编程语言，用户可以通过它查询数据库中的复杂信息。这种语言脱胎于关系代数和微积分，全称为结构化英语查询语言（SEQUEL，即Structured English Query Language），后来又很快简称为结构化查询语言（SQL）。SQL支持查询、表达式、从句等多种编程元素。有了它，用户就可以在数据库中查询高度复杂的信息。数据库管理系统负责接收和翻译SQL表达式，并尽快返回结果（如果情况理想的话）。SQL中包含好几种语言，其中数据操纵语言（data manipulation language，简称DML）和数据定义语言（data definition language，简称DDL）能让用户运用简单易用的表达式在数据库中修改或添加数据。此后，数据库在原有的基础上继续改进，开始向面向对象的趋势发展，并进一步得到优化。索引的概念得到引进，经常查询的信息存储在高速的临时数据库中，临时数据库则与主数据库相连。正如图书目录一样，索引可以使用户查找所需信息的速度大幅加快。如今，数据库的设计特点表现为高度模块化，可访问性强。可扩展标记语言（Extensible Markup Language，简称XML）、超级文本预处理语言（Hypertext Preprocessor，简称PHP）等基于网页的语言可以使用SQL，连接在线数据库。（有了网页服务器，当你浏览的网页中包含这些高级语言写成的命令时，网页浏览器就会对它们进行翻译。）可以想见，未来数据库的分布将日趋广泛，可访问性日益增强，访问速度日益加快。 我们已经对数据库领域的巨大飞跃感同身受。就在几年前，家家户户的抽屉里还塞满了老照片，书架上堆满了书本、文件、录像带和CD，但是如今，我们查找、创建、购买的所有信息都是以数字形式保存下来的。大多数照片都已实现数字化，电子书正日渐占据图书市场的主流。大多数公司都喜欢给客户邮寄电子账单，而非纸质账单。视频和音乐已大多实现数字化。多年来，我们的财富不过是存储在银行计算机中的数字。政府以数字化的形式发布信息；医院在计算机上保存病例。越来越多的商店开始实行网上交易。我们只需要轻轻点击几次鼠标，就可以购买食品、汽车乃至任何种类的商品。所有这些海量的数据都存储在数据库中。它们的界面可以是一个赏心悦目的网页，供人随意浏览各种美观的照片；可以是一家网上商店，供人随心挑选品类丰富的商品；可以是一款功能强大的音乐播放程序，能让人徜徉于音乐的海洋；甚至可以是一款会计应用程序，能让人随时监测开支情况。但是，在多姿多彩的界面背后，让这一切成为可能的，便是精巧的数据库技术。 软件危机如今，软件业可谓风生水起，蔚为大观，仿佛这个行业的发展向来一帆风顺，毫无波澜。但是，正如电子业历经了“数字暴政”的阵痛，才有了集成电路的诞生，新生的软件业也经历了危机的考验，这场危机还推动了一门全新学科的诞生。 编写大型程序的巨大困难程序员遇到的问题从一开始就很明晰。虽然有了新的高级编程语言的帮助，开发人员在编写代码的过程中可谓得心应手，但是，前所未有的重大问题依然在不断涌现。20世纪60年代，随着集成电路的发明和摩尔定律的问世，计算机的性能每年都在突飞猛进。与此同时，软件应用也在以类似的速度不断飞跃。然而，就在这个过程中，程序员日益强烈地意识到，他们的程序正变得越来越难以掌控。程序当中的错误太多，软件并没有发挥应有的效果，而且开发系统的过程似乎正变得意料之外地漫长。 这些问题在1968年达到了顶点。在有史以来的第一次软件工程大会上，世界各地的计算机科学家齐聚一堂，共同讨论他们关切的问题。他们忧心忡忡，这一点从会上的讨论中就可以看出来： 麻省理工学院的罗伯特·格雷厄姆（Robert Graham）表示：“我们投入了长年累月的研究，耗费了巨大的投资，到头来却发现，我们从一开始就没有把系统研究透彻，软件根本没有取得预期的效果。我们就像莱特兄弟制造飞机一样，辛辛苦苦地把飞机造好，将它推下悬崖，任凭它轻而易举地坠毁，然后再从头开始。”密歇根大学计算中心（University of Michigan Computing Center）的伯纳德·加勒（Bernard Galler）表示：“我想举几个在IBM碰到的不好的例子。有一次，用户提出，希望能够增强PL/1语言可扩展性。对此，IBM经过一周的内部讨论，最终下结论称，这种事情不可能做到。因为语言设计师不打算告诉用户怎样实现所需的扩展。还有一个例子：OS/360操作系统的作业控制语言（job control language，简称JCL）开发出来以后，用户在设计阶段根本无法事先看到任何选项。为什么会出现这种情况呢？”丹麦第一家计算机公司A/S Regnecentralen的员工彼得·诺尔（Peter Naur）表示：“……软件设计师的角色类似于建筑师和土木工程师，尤其是规划城市、工厂等复杂建筑的设计师。因此我们应该学会从这些领域吸取灵感，攻克我们遇到的设计问题。” 此次大会落幕后，另一场大会很快召开，其宗旨是讨论技术和管理思想。从这一刻开始，软件工程学作为一门新的学科登上了历史的舞台。伊恩·萨默维尔是现代软件工程师、英国圣安德鲁斯大学（St. Andrews University）教授，他撰写了很多业内的权威教科书。他表示，新学科的命名颇有内涵，意在表明，人们从此将采取系统的、有组织的方式来编写软件。不过，大会的主办方给出了不同的说法，“他们声称之所以发明这个术语，只是出于调侃的心态，没想到这种叫法就传播开了”。当然，这样命名是为了“故意制造煽动的效果”，以激发研究人员行动起来。还别说，这样做真的起到了效果。没过多久，许多关键性的技术革新开始涌现，它们专门针对的是软件工程学领域，旨在辅助程序开发人员提高编程能力，写出高效的软件。在这一理念的指导下，新的编程语言得到开发。戴维·帕纳斯（David Parnas）等研究人员提出了信息隐藏的概念，这一概念在模块化编程和面向对象的编程领域举足轻重，它可以确保数据及相关函数封装在对象内，与其他的数据和函数分隔开来。这就好比采用标准化模块制造汽车——车载收音机等部件的更换不会影响到其他部件。同理，如果你设计好程序之后，突然想对某个地方进行改写（比方说将某个函数或数据结构更换），那么程序中的其他函数和数据完全不会受到影响。集成开发环境（Integrated development environment，简称IDE）的发明，就是为了让编程变得更加轻松。它们的作用就好比文字处理器——程序开发人员可以在不同的窗口编写代码，并对其进行编译和调试。正如文字处理器可以检查拼写和语法错误，集成开发环境也可以帮助编程人员找出程序中的错误，并提供大量实用的工具进行除错。现代的集成开发环境通常包括一系列工具，比如：手持设备模拟器；用于设计图形用户界面（Graphical User Interface，简称GUI）的工具；全方位的帮助系统，用于辅助编程人员从已有的库——即程序编程接口（application programming interface，简称API）中寻找合适的函数。 如何面向用户除了发明实用的编程工具，研究人员很快意识到，还有更好的软件设计方法亟待开发。这就好比汽车制造商不能把仅仅把目光放在硬件上，还应着眼于汽车的用途、目标客户的定位、成本开销的大小，从而将生产问题化整为零，分解成一个个具体的问题，比如：应该采用什么样的发动机、传动部件、转向系统、制动系统、车轮系统和座椅系统？车内需要容纳多少人？生产一台汽车需要多少时间？编写软件也是同样的道理。一个大型软件项目可能会比制造汽车复杂得多，怎样设计才能确保项目的高效运转？研究人员很快意识到，要做到这一点，必须确立一个明确的软件生命周期。首先，你必须合理定位产品和项目需求。接下来要做的，就是设计、运行和测试软件，并将其运行情况清晰地记录下来。最后要做的，就是发布软件，或许在这个阶段，你还需要指导用户如何高效地使用软件、如何进行必要的维护工作。 这些事情说起来容易，做起来难。产品或项目的需求并不好确定，因为顾客往往并不知道自己真正的需求到底是什么，因此可能会举棋不定、自相矛盾，甚至改变主意。他们往往不具备编程人员的思维方式，因此不知道如何从软件开发的角度表达自己的需求。换句话说，对于哪些事情在技术上可行，而哪些不可行，他们基本上没什么概念。有的时候，编程人员连目标用户是哪些人都无法确定，因此，真正应该提要求的人反而没有这个机会。 如何规划大型项目除此之外，不同的设计阶段应该如何开展，这也是摆在编程人员面前的一大难题。项目的开发架构是应该采用“瀑布模型”（waterfall model）——像流水下坡一样顺次开展每个阶段，还是应该采用“螺旋模型”（spiral model）——反复开展每个阶段，开发一系列原型软件，以最大限度地降低风险？软件的开发是应该遵循迭代式和增量式的过程，还是应该采用“灵活性强”的方式，以迅速适应可能发生的变化？就算你知道了开发软件的最佳顺序，怎样才能把每一个阶段的工作都做到最好？应该采用哪些设计方法和工具？怎样测试软件，才能确保万无一失？怎样维护软件，才能使之适应未来的变化？ 正因为软件工程学专家孜孜不倦地攻克上述难题，软件项目的发展才得以适应硬件的发展需求和用户复杂的使用需求。软件工程学着眼于软件开发的方方面面，比如对软件体系结构进行建模，采用可视化程序设计，使用形式化方法测试软件、提高其性能的可靠性。伊恩·萨默维尔认为，软件工程学领域已经取得了许多重大的进步，这些进步给软件开发带来了实实在在的影响。“开发不同类型的软件，需要采用不同类型的软件技术和方法，”他表示，“配置管理（Configuration Management，简称CM）就是一个非常重要的项目管理方法，它支持并行开发。信息隐藏的概念是由帕纳斯在1972年提出的，它在抽象数据类型（Abstruct Data Type，简称ADT）领域得到了进一步发展，并影响了大多数现代编程语言。在关键任务系统领域，我们不仅可以采用安全分析和可靠性分析的方法，还可以运用容错技术，为航空器、化工厂等重要设施建立安全可靠的系统。统一建模语言（Unified Modelling Language，简称UML）将多种建模概念融合在一起，如今已成为软件系统建模的标准方式。编程环境的思想于20世纪70年代问世，在20世纪80年代得到进一步发展，如今已在软件工程学界得到广泛采纳。”安东尼·芬克尔斯坦是软件工程学教授，在伦敦大学学院担任工程学系系主任。他认为，正因为有了软件工程学的帮助，程序员才得以高效利用时间。“如果没有软件工程学，我认为硬件与软件之间的性能差距会进一步扩大。制约软件开发的主要因素就是缺少训练有素的人才，现在也是如此。如果当初没有足够的人才投身于软件工程行业，我们就无法取得今天的成就了。” 软件膨胀不过，尽管为数众多的软件工程师竭尽全力地投身于技术攻坚，但是依然无法解决所有的问题。我们每次使用计算机时，都会对软件工程领域的一大问题感同身受：由于某种原因，软件每次升完级以后，其运转速度似乎都会变慢。计算机科学家（及众多编程语言的发明者）尼古拉斯·维尔特（Niklaus Wirth）观察到了这一现象，人们将其称为维尔特定律（Wirth’s Law）。维尔特定律的内容是：软件变慢的速度永远快过硬件变快的速度。其他科学家也发表过类似的观点。曾在英特尔担任研究人员的兰德尔·肯尼迪（Randall Kennedy）就是其中一人。他曾写道：“虽然与几年前的Office 2000相比，Vista系统上的微软的Office 2007 虽然处理能力提高了将近两倍，但是占用空间却多出了11倍以上。”造成这一现象的罪魁祸首是软件膨胀（software bloat）——新版本的软件往往只是在原版本的基础上叠加了新的代码，而并没有经过重新编写。 维尔特定律表明，纵使计算机的运行速度快得惊人，新一代的软件的运 行速度也比不上十年前的老版本。目前，计算机科学家正在费尽心思解决这个问题。不过萨默维尔深知，这并不是软件工程学领域的唯一挑战。“软件项目面临的主要挑战在于，开发环境正变得越来越复杂。这是因为，我们在建立新系统的过程中，将不同的供应商提供的各种系统和服务整合了起来。原本，在软件工程学领域，很多理论之所以能够成立，往往是基于这样的前提，那就是，系统完全处在软件开发人员的掌控之下，软件开发人员可以做出明智的决定来开发和改变系统。当这样的前提不再成立时，软件测试等方法必须做出相应的调整，以适应新的情况。”这一观点引起了一些计算机科学家的高度重视。伦敦大学学院教授马克·哈曼围绕搜索问题对软件工程学进行了研究。他按照遗传算法——也就是由“优胜劣汰”的生物进化规律演化而来的随机化搜索方法，利用计算机来寻找特定软件的最优测试方法。“软件测试是衡量软件质量、寻找优化方法的关键手段之一。测试的内容之一，就是寻找特定的输入，使程序执行特定的代码片段，”哈曼表示，“人工完成这样的工作需要花费很大的心血。这就好比你得从厚厚的地址簿里大海捞针，将一个人的电话和住址找出来。但是，如果计算机能够对测试用例进行评定，我们可以轻而易举地将这个过程自动化。” 其他危机不过，仅仅有个聪明的测试方法是不够的。我们所说的危机与20世纪60年代末的软件危机已经无法同日而语，但是从某种程度上讲，软件业依然面临着定位问题。大型软件开发项目依然经常失败，尽管软件工程师尽了最大的努力。有时候，问题只是出现在成本和时间上——我们并不擅长估算软件开发产生的耗费。“我们的软件估算水平很差，”芬克尔斯坦表示，“如果你让我‘开发一个售卖二手教科书的网页前端，’我可能一时半会儿没法告诉你需要多少时间。运气好的话，我可能之前就做过一次这样的项目，但是我在这方面的经验也就仅限于此了。’”不过，重大的失败也有可能是由更微妙的原因造成的（而且这样的事情不在少数，造成的损失也极为惨重）。芬克尔斯坦认为，问题的症结并不在于分析师和开发者经验不足。“个人认为，这些系统之所以失败，都是一些人们很熟悉的问题造成的：比如在项目设计和开展阶段犯了低级错误，”他表示，“你必须问自己，既然人们对这些问题都很熟悉，为什么还要一次又一次地犯同样的错误？他们是傻子吗？他们没看过萨默维尔的书吗？这些问题前十页就讲了，哪怕稍微翻翻也好啊，又不一定要全读完！我觉得，问题的症结是，企业结构、管理决策结构与开发软件的技术过程存在不协调。因此，这些系统之所以会失败，问题出在管理上，而不是工程上。”软件工程领域的挑战还表现在，怎样满足客户的信息安全需求，怎样设计软件才能使之适应互联网时代的潮流。此外，一些所谓的“非功能性需求”也越来越值得重视，比如电源或电池寿命。在这个问题上，哈曼的想法比较现实：“如果我在乘坐越洋航班时，笔记本的电量在半路上就耗光了，那么就算里面的软件再好，对我来说一点用也没有。我宁愿软件耗电量低点，让续航时间久点，就算软件里头到处都是漏洞也没有关系。”芬克尔斯坦也这么认为：“在软件工程领域，我们一直致力于开发新特性，从来没有想过省电的问题——一般都不会有人往这方面想，除非你要设计航天器。但是现在，这个问题已经变得很关键。有很多之前没有考虑过的新特性一下子变得非常重要。” 还记得以前我们讲到的编写并行软件吗？那是帕特森给整个行业的科学家提出的挑战。芬克尔斯坦确信，它会给软件工程领域带来重大的变革。“软件工程领域面临的另一大挑战就是多核技术。如果软件无法充分发挥多核技术的优势，那么一台计算机就算内核再多也于事无补。要是采用一个内核就足以运行所有软件，那么剩下15个内核留着干嘛呢？总不可能都用来运行杀毒软件吧。我们所需要的，不仅仅是编程技术的一个突破，而是整个软件工程领域的革新。这肯定会改变我们当前的游戏规则。”]]></content>
      <categories>
        <category>计算机</category>
        <category>历史</category>
        <category>软件</category>
      </categories>
      <tags>
        <tag>计算机</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机先驱——莫里斯·威尔克斯]]></title>
    <url>%2F2019%2F05%2F19%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%85%88%E9%A9%B14%2F</url>
    <content type="text"><![CDATA[Sir Maurice Vincent Wilkes，1913年6月23日－2010年11月29日1931年他进入剑桥的圣约翰学院，之后进入剑桥著名的卡文迪什实验室工作。于1938年10月取得剑桥大学博士学位，而他的硕士学位是在当年年初才取得的。英国计算机科学家。设计和制造了世界上第一台存储程序式电子计算机EDSAC，在“工程和软件等计算机领域都有许多开创性成果”。 开端这是1946年的一个温暖和煦的仲夏日，时值星期一早晨。回想起三年前与图灵在下午茶时间的热烈讨论，克劳德·香农觉得恍如隔世。此时此刻，他正在宾夕法尼亚大学的摩尔电气工程学院学习一门为期八周的课程，这门课目前已经上到了第三周。能够受邀来此听课，对看他来说已是一大荣幸，毕竟，这是少数人才有的殊荣。课程采用讲座的形式，主题是电子数字计算机的设计。这是世界上第一门关于计算机科学的课程，香农发现，课上所学的许多思想在他的头脑里擦出了灵感的火花。他最近从莫奇利那里学到了一个新词——“编程”。它通常作为动词使用。给电子计算机编程是一个让人耳目一新的概念。香农还听到了一些关于办公室政治的八卦：给他们开课的两位讲师——莫奇利及其同事埃克特四个月前刚从宾夕法尼亚大学辞职，原因似乎是围绕“埃德瓦克”产生的专利纠纷。另一名讲师戈德斯坦近期就要去高等研究院（IAS）入职了，届时他将与香农的老同事约翰·冯·诺依曼共事。冯·诺依曼本来也要到这里来开设两个星期的讲座，但是他好像不会来了，因为临时有事。 在一篇机密级的论文中，香农发布了他最近在贝尔实验室开展密码学研究的部分成果。这些研究成果与他当年攻读理科硕士期间提出的思想一脉相承，只不过二者的着眼点不同。香农当年着眼的是开关电路，现在着眼的则是密码学背后的数学原理。他开始意识到，破译扰频加密的信息，其实大体上就相当于给正常传输的信息纠错。举个例子，如果一段编码中出现了重复信息——比方说它的内容中包含许多常见的词语如“这个”、“一个”、“和”，那么这段编码破译起来就会轻松许多。因为我们知道，“这个”、“一个”、“和”这类简短的词语是句中常见的语法成分，就算把它们的每个字都改头换面，也不难根据它们在句中出现的位置来判断其成分。只要破译了高频词，那么整条加密信息指的是什么内容，或许就能猜出个大概了。因此，要想增加破译难度，必须尽量减少冗余信息。但是，如果你想尽可能多地保留原始信息，而记忆存储系统又很容易出错，那就最好少删除一些冗余信息。检验信息是否出错的方法之一，就是设置奇偶校验位（parity）。至于奇偶校验位是指什么，还得从比特的概念说起。比特（bit）是内存中的最小单位，也称作“位”、它只有两个状态，分别以1和0表示。我们将8个连续的比特叫做一个字节（byte），比如（1、0、0、1、1、1、1、0）就是一个典型的字节。如果其中某一位存储了错误的值，那就会导致信息出错。为了检测信息是否出错，我们在每一个字节（8位）后面又额外增加了一位，称为奇偶校验位。这样一来，原来的8位字节就变成了新的9位字节。奇偶校验位也只有1和0两种值。如果原字节中1的个数为奇数，那么奇偶校验位就设为1，这样一来，新字节中1的个数就变为偶数；反之，如果原字节中1的个数为偶数，那么奇偶校验位就设为0，这样一来，新字节中1的个数依然为偶数。也就是说，凡是带有奇偶校验位的字节当中，1的个数始终应该为偶数，如果你发现某个字节不是这样，那就说明它有错误，这段字节包含的信息就需要重新读取。（这个方法在20世纪50年代早期开始在计算机领域广泛采用，后来，人们很快就开发出了更多高级的方法。） 言归正传，香农在摩尔学院听讲座的过程中，了解了二进制对于计算机的重要价值：二进制数不仅是计算机内各个部件交换信息的重要载体，还是确保信息存储和检索过程不出差错的重要工具。后来，香农发明了术语“比特”来指代二进制数位（binary digit），同时阐述了如何利用比特来衡量信息量，并对信息进行传输、加密、压缩、纠错。在当年赴摩尔学院听讲座的人当中，香农并不是唯一一个计算机领域的先驱。莫里斯·威尔克斯也去了摩尔学院，只不过他差点就错过了这门课程。香农在台下听莫奇利讲解二进制与十进制数的那一天，威尔克斯还在英国。1945年，莫里斯·威尔克斯在剑桥大学听说了埃克特和莫奇利在美国研制埃尼阿克的工作，于是在1946年2月给学院提交了一份报告。他在报告中写道：“这是一个大有可为的研究领域，电子应用技术首当其冲。它在战时取得了迅猛的发展。 美国人在这门学科上已经领先了一步，我觉得剑桥也应该迎头赶上。”三个月后，机械计算机专家莱斯利·科姆里（Leslie Comri）来访剑桥。他从美国带来了一份手稿副本——即约翰·冯·诺依曼所写的《关于埃德瓦克的报告初稿》。威尔克斯只有一个晚上的时间阅读报告，当时还没有影印机，他只能边读边作笔记。威尔克斯很快就被报告的内容吸引住了。“我很快就意识到这个研究成果非同小可，”他表示，“从那以后，我对计算机的发展前景一直没有怀疑过。”就在威尔克斯依然对报告的内容记忆犹新之时，他突然接到了摩尔学院院长哈罗德·彭德（Harold Pender）发来的电报，邀请他去参加一门新开的电子计算机课程。威尔克斯的越洋航程并不顺利。尽管伙食条件一流，但是住宿条件太差，35个人挤在一艘只能容纳20个人的小货船里。更糟糕的是，发动机在中途抛锚了好几次。就这样，威尔克斯一路历经磨难，终于在8月15日抵达了纽约，上岸后又马不停蹄地赶路，总算在8月18日赶到了费城。这时候，他已经错过了三分之二的课程，好在前面的课程大多都是些入门性质的讲座。到了8月19日，也就是星期一，威尔克斯抵达摩尔学院，正好赶上当天下半节课。这堂课讲的是埃尼阿克的细节内容，讲师提供了详尽的电路图。威尔克斯回到剑桥大学，满脑子都是计算机领域的前沿思想和美国人取得的一些关键成果。威尔克斯认为，是时候研制一台实用的存储程序计算机了。幸运的是，没过多久，餐饮巨头J. Lyons &amp; Company就给他提供了科研经费和技术人员，因为该公司需要计算机进行会计核算，管理员工工资单。威尔克斯的项目进展很快，他在雷达领域积累的经验更是大大加速了这一进程。项目团队在此基础上构建了一个工作记忆系统，用于存储数字。威尔克斯在研制新机器的过程中，旁听了图灵开设的几个讲座，讲座的主题是图灵关于自动计算机的设计思想。不过，两个人的设计理念并不一致。图灵认为，计算机应该在水银延迟线存储器的基础上进行优化设计。威尔克斯的观点正好相反。“我认为，水银延迟线存储器迟早要被真正的随机存取存储器淘汰。与其把时间和精力都花在一项短命的技术上，还不如多下点功夫研究编程，毕竟，编程领域还有那么多问题值得研究。”图灵也不赞赏威尔克斯的设计理念，他在一篇备忘录中写道，威尔克斯有一些理念“比较偏向美国传统，遇到什么困难就喜欢依赖设备，而不喜欢动脑子。”但是事实证明，威尔克斯的方法更加实际。剑桥大学的电子延迟存储自动计算机（Electronic Delay Storage Automatic Calculator，简称EDSAC）于1949年5月6日投入运行，直到1958年才光荣退役。它是世界上第一台实用的存储程序计算机。 学习计算机编程剑桥大学的EDSAC计算机并不只是一台前沿尖端的机器，它还开创了计算机领域的先河。此前的计算机每次执行新的运算，都需要插入不同的线路进行重新装配，而EDSAC则通过存储器中的软件实现各种不同的运算操作，这就对编程提出了很高的要求。为了写出功能强大的软件，威尔克斯和戴维·惠勒（David Wheeler）等研究人员提出并改进了许多新的思想，时至今日，这些思想在计算机编程领域已占据主流地位。不过编程在当时并不是一件容易的事情。所有的早期计算机先驱很快就意识到，一旦设计出存储程序计算机，就必须拼了老命地编写计算机能够运行的程序。如果任何程序都无外乎是一组能够触发数学或逻辑学电路的二进制数，那么编写软件就会变成一场噩梦——而事实也的确如此。1949年6月，惠勒第一次意识到了编程的艰难。他后来回忆起了当时的情形：“那时候，我正试着让自己编写的第一个真正意义上的程序运转起来。有一次，我像往常一样从EDSAC机房出来，准备去操作打孔机，突然站在楼梯转角处犹豫了，心里意识到，单是给自己的程序除错，可能就要花掉我大半辈子的时间。”显然，当时的科学家需要一些新的思想，来简化编程过程，提高编程能力。对此，威尔克斯（在几年后）提出了一个方法，称为微程序设计。当时，麻省理工学院正在研制的旋风计算机（Whirlwind）给了他部分灵感，让他意识到，并不是每一条低级指令——比如除法——都需要电子电路来执行。复杂的指令完全可以分解成一系列简单的指令，而微代码编写出来的微程序可以作为二进制机器代码和硬件之间的桥梁。事实证明，这一方法实用性很强，时至今日，复杂指令集计算机（Complex Instruction Set Computer，简称CISC）处理器依然采用了这一思想原理，以执行高度复杂的操作。而精简指令集计算机（Reduced instruction set computer，简称RISC）处理器则不使用微编程。但是即便是使用微代码，计算机编程人员依然需要编写一长串的数字，即机器代码指令。而且，早期的编程人员还因为机器的内存容量极其有限而备受掣肘。EDSAC的内存只有两千字节左右（放到今天，一部手机的内存都比它大几百万倍）。为了解决这些问题，威尔克斯的团队又想出了一个妙招——编写子程序。研究人员意识到，许多程序在运行的过程中，都需要重复执行某个操作——比如在某个复杂的数字运算中，需要多次进行开平方操作。如果每次开平方都得把平方根代码写上，那么程序当中就会出现许多重复代码，占用不必要的空间，使程序变得庞大而低效。这就好比你在写一个句子时，不仅构造了完整的语法结构，还将句中每个词语的定义也写了下来。为了简化编程过程，威尔克斯的方法是建立子程序库，也就是将常见的函数单独列出，集中起来，就像把常见的词语及其释义收录在词典中一样。一旦程序在运行的过程中需要使用到某个常见函数，计算机就会在子程序库中“查找定义”，执行相应的子程序代码，根据输入值进行运算，再将运算结果返回。在这一方面，威尔克斯的理念已经领先于同时代的大多数人。冯· 诺依曼有一次突然造访，与威尔克斯进行了讨论。两个人的观点产生了分歧。威尔克斯回忆道：“他觉得应该把开平方运算嵌入到计算机的指令集中……我自己的立场则有所不同，我已经把子程序看做是对基本指令集的扩展，所以觉得没有必要再在指令集中嵌入一个特殊的函数。”由于威尔克斯领导的剑桥大学研究团队很早就开始在编程领域开疆拓土，他们对编程的艺术也颇有心得。普林斯顿高等研究院的冯·诺依曼喜欢绘制“程序框图”，来展示程序应该如何运作。所谓程序框图，大体上就是一系列带箭头的步骤，可以展示控制流。不过，威尔克斯手下最好的程序开发人员戴维·惠勒认为，要想写出出色的软件，还需要其他的方法。“我们关心的首要问题是，用户在操作过程中是否得心应手，因此在这方面也下了很大的功夫。”他表示，“模块化设计的编程风格很早就开始得到推行。这是我们开展编程教学的方式。当然，我们也知道一些其他的方法，比如冯·诺依曼的程序框图。不过这些方法一般都不用，就算用也只是拿来说明已经完成的步骤。我 们发现，只要将复杂的问题进行分解，用一个个子程序加以解决，然后将子程序置于主程序的控制之下，就可以逐步形成模块化设计的思维方式，到时候程序设计自然水到渠成，根本不需要使用到程序框图。个人认为，程序框图最容易让人写出垃圾软件。并不是说这种方法毫无价值，只不过你要是想拿它来代替思考过程，根本就不管用。”威尔克斯的团队还考虑到了程序员可能遇到的其他困难。他们从一开始就意识到，尽管计算机只能理解数字，但很少有人能够在只采用数字的情况下编写程序，了解计算机的运行过程。人的大脑习惯了阅读文字和符号，而不是处理一连串数字。惠勒认为，冯·诺依曼在高等研究院研制的计算机在这一方面做得很差，完全比不上剑桥大学的 EDSAC。“它太原始了，这让我非常震惊，”他表示，“我估计它的程序是用二进制输入的。我们剑桥大学的研究团队很早就开始采用了一种叫做‘汇编程序’的工具。”它能够转换编程语言，从十进制转为二进制，使用助记符，可引用代码，可分隔字段，可自动定位子程序，还具备其他各种功能。我们基本上从开展项目的第一天起就已经在使用汇编程序了。这极大地简化了编程过程。这样一来，程序开发人员就不需要和抽象的二进制数打交道了，他们可以采用简短的词语来编写程序，这些词语看起来有点像英文单词。与一连串抽象的二进制数字相比，即使是奇怪而又晦涩的文字无论在阅读还是理解上，都要轻松许多。它与处理器使用的低级的代码没有太大的分别：其中的每一个词语（或命令）——比如“cmpl”、“jmp”——都直接对应于机器代码中的一条指令。要将汇编语言写成的程序转换为相应的机器代码，就需要使用到另一种计算机程序，称为汇编程序。汇编程序读取的是用汇编语言书写的源程序，输出的是用机器语言表示的目标程序。汇编语言在计算机领域举足轻重，20世纪90年代的所有程序都是由汇编语言写成。几十年来，有两种常见的程序只采用汇编语言：一是计算机游戏（因为开发人员希望尽可能地提高游戏的运转速度，同时尽可能给玩家带来极致丰富的游戏体验），二是操作系统。即便是在当今时代，程序员要想编写速度超快而形式简洁的代码，都免不了要采用一些汇编语言。 攀登更高峰1951年，计算机开始搭载好几个层次的软件。第一层是微代码，它完全依赖于芯片内部的硬件连接。第二层是机器代码，它比微代码更抽象一些。第三层是汇编语言，它比机器代码可读性稍强。计算机编程，说白了其实就是告诉计算机应该使用哪种逻辑和算术电路。要想给计算机编程，程序员可以使用汇编语言来写代码，而汇编语言正如我们在上一节所看到的那样，和英文单词有些类似。这些代码随即由汇编程序转化为机器代码，机器代码定义微程序的指令，微程序则在算术和逻辑单元（ALU）的电子元件中被翻译为一系列指令的组合。 不过，汇编语言对于许多程序员来说依然难度太大。如果你想处理更复杂的思想和概念，那么，纠结于个别的跳转指令只会拖累你的步伐。 如果你希望自己的程序能够在完全不同的处理器上运行，那就需要采用高级的编程语言，也就是独立于底层硬件的计算机语言。1953年，随着电子计算机在全世界遍地开花，发明抽象编程语言的问题开始受到广泛关注。莫里斯·威尔克斯受邀主持了美国计算机协会（ACM）在麻省理工学院召开的一期研讨会，专门探讨这个问题（当时的会议主题为自动编程）。威尔克斯至今还清楚地记得当年会上的讨论情况。“与会者的意见分歧相当尖锐。有些人认为，凡是试图绕开困难的做法都是误入歧途。程序员只有老老实实地恪守本分，编程领域才会取得更大的发展。另一方面，还有一些人认为，只有新的技术才具有实实在在的实用价值。”（时至今日，计算机科学家当中依然存在这两派的纷争。）许多研究人员已经在试验新的、更简单的编程语言。早在1949年，约翰·莫奇利就发明了一种语言，称为简代码（Brief Code），后来更名为短代码（Short Code）。短代码虽然使用起来简单许多，但是需要翻译——也就是说，计算机每次运行这种语言编写的程序，都得临时将短代码翻译成机器代码。这就意味着，这种程序的运行速度比机器代码写成的程序慢54倍。与此同时，在英国曼彻斯特，一位名叫埃里克·格伦尼（Alick Glennie）的研究人员发明了另一种语言。这种语言易于使用，可通过另一种程序自动转换为机器代码，因此具备简单实用、运行速度快的双重优势。格伦尼将其称为自动代码（Autocode）。 显然，用自动代码写程序比用汇编语言要轻松许多。其中有些自动代码语句和好几条机器代码指令相对应，不过程序员不需要为此挂心。只要使用另一种称为编译器的程序，就可以将这段由自动代码编写而成的简洁英文命令翻译成机器代码。自动代码是世界上最早出现的编译型高级编程语言。自计算机协会在麻省理工学院召开研讨会，讨论自动编程问题后，越来越多的计算机科学家开始意识到，编程过程的简化势在必行。1957年，IBM的约翰·巴库斯（John Backus）发明了另一种编译型高级编程语言，称为福传（FORTRAN）。福传的全名是Formula Translation，意思是“公式翻译”。这种语言甚至比自动代码更高级，可以用来编写更加复杂的程序。它的编译器也极为智能，可以生成非常简洁高效的机器代码。没过多久，许多适用于其他计算机的福传编译器相继问世，这样一来，同一款福传程序就可以编译成不同计算机所特有的机器代码。从这一刻开始，程序员便有了一种新的工具——“便携式”代码，它可以使同一款程序在完全不同的计算机上运行。很快，其他编程语言也开始纷纷效法，比如算法语言（ALGOrithmic language，简称ALGOL）、列表处理语言（LISt Processor，简称LISP）、初学者通用符号指令码（Beginner’s All-purpose Symbolic Instruction Code，简称BASIC）。随着计算机设计师开始研制晶体管计算机，这些语言也得到了稳步改进。很快，计算机科学家便能分析编程语言，并用数学方法——包括邱奇的λ演算将其形式化。程序员也可以在代码中表达更抽象的概念，而不需要操心低层次的细节问题。各种类型的编程语言相继问世。早期语言基本上都是过程式语言（程序员告诉计算机如何执行过程步骤），后来的语言则采用了不同的编程范型。在面向对象的语言（Object-Oriented Language）中，数据及其操控方法都封装在“对象”中，以实现代码的模块化，防止数据的意外损坏（这一点也是程序的“副作用”）。函数式编程语言利用若干简单的执行单元让计算结果不断渐进，逐层推导复杂的运算，而不是像过程语言一样，设计一个复杂的执行过程。此外，还有更多适用于并行计算机的程序语言相继问世。正因为众多早期先驱的开创性工作，如今人们习以为常的一些重要编程思想才得以诞生。威尔克斯继续投入计算机语言的研究，他在改良算法语言60（ALGOL 60）的基础上发明了CPL（Combined Programming Language）编程语言。这种语言并没有受到热烈的反响，但是却奠定了BCPL（Basic Combined Programming Language）语言的基础。BCPL进一步发展演变，推动了B语言和C语言的问世。直到现在， C语言（以及在此基础上形成的诸多语言，如C++、C#、Objective C）或许是世界上应用最广泛的计算机编程语言之一。人们当前使用的许多众所周知的操作系统（比如UNIX、Linux、Mac OS X、Windows）都是用C语言写成的。如今，几乎每一台计算机上都搭载了C语言编译器，方便用户使用C语言编写代码。]]></content>
      <categories>
        <category>计算机</category>
        <category>历史</category>
        <category>先驱</category>
      </categories>
      <tags>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机先驱——罗伯特·诺伊斯与戈登·摩尔]]></title>
    <url>%2F2019%2F05%2F18%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%85%88%E9%A9%B13%2F</url>
    <content type="text"><![CDATA[Robert Norton Noyce，1927年12月12日—1990年6月3日1953年，获麻省理工学院（MIT）博士学位；1949年，获格林尼学院文学学士学位。1968年创办英特尔公司、1957年创办仙童半导体公司。 Gordon Moore，1929年1月3日—？加州大学伯克利分校的化学学士学位，并且在加州理工学院（Caltech）获得物理化学（physical chemistry)博士学位美国科学家，企业家，英特尔公司创始人之一。 摩尔定律发明硅芯片后，罗伯特·诺伊斯继续和同事戈登·摩尔（Gordon Moore）在飞兆半导体公司（Fairchild Semiconductor Corporation）研究集成电路技术，直到1968年，两人成立了一家新公司，即著名的英特尔。他们和全世界的业内先驱一道，共同掀起了电子学领域的技术革新。早期的集成电路还只有几百个、乃至上千个晶体管，但制造工艺的稳步改进使单个芯片上可以容纳的晶体管数目越来越多。（20世纪60年代早期，集成电路技术之所以迅猛发展，很大程度上是因为美国导弹计划和阿波罗太空计划的推动。）集成电路的日益复杂使戈登·摩尔在1965年做出了一个预测。当时，他注意到，从1958年集成电路问世到1965年，单个芯片上的晶体管数量每年都翻了一倍。于是，他预测，这个趋势至少还会持续十年。后来，他修正了自己的观点，认为单个芯片上的晶体管数量每两年就会翻一倍。1970年，加州理工学院（California Institute of Technology简称 Caltech）教授卡弗·米德（Carver Mead）发明了专门的术语，将这个预测称为“摩尔定律”。令人惊叹的是，这个“定律”似乎一直都很准。单个芯片上可容纳的晶体管数量在20世纪70年代中期为一万个，在1986年达到了一百万个，在2005年则为十亿个。尽管经常有人提出，摩尔定律很快就会失效，因为晶体管的尺寸越做越小，已经快要达到物理定律的极限，不过到目前为止，这项卓越的技术依然保持着强劲的发展势头。计算机技术一向与电子学的前沿技术联系紧密，因此从20世纪60年代开始，硅芯片技术的迅猛发展也带动了计算机的更新换代。摩尔在英特尔的同事大卫·豪斯（David House）认为，从摩尔定律可以推断出，计算机的性能每隔18个月就会翻一倍。他说的基本没错——这些年来，计算机的性能大约每隔20个月就翻了一倍。摩尔后来开玩笑说：“18个 月是豪斯说的，不关我的事。” 正因为电子学正向着微型化的方向大幅迈进，我们的计算机每年都在变得更小巧、更便宜、更强大。早期的计算机都是庞然大物，很多情况下只能靠远程终端控制（一台大型计算机就需要很多个键盘和显示屏）。很快，小型计算机和个人计算机（台式机）相继问世。随着单一芯片上集成的元件继续增多，计算机的尺寸进一步缩水，由此产生了便携式个人计算机，也就是笔记本。后来又出现了更迷你的上网本、平板计算机和掌上设备，比如智能手机。计算机已变得小巧、廉价，足以在儿童玩具上实现复杂的功能，甚至在贺卡中嵌入音乐，而且由于它的更新换代速度太快，被淘汰的设备无需多想就可以直接丢弃。本已小巧玲 的计算机还会向着更小巧、更便宜的方向迈进，这种发展趋势还没有任何减缓的迹象。虽然有了摩尔定律这一强大利器，科学家并没有安于现状，不再绞尽脑汁寻找提升计算速度的新方法。事实上，计算机领域的创新远未止步。尽管所有计算机都具备冯·诺依曼当年在报告中提到的逻辑元件，但它们远非千篇一律。为了提升运算速度或效率，各种高明的优化设计方案层出不穷。举个例子，自从计算机问世以来，工程师就一直面临着两难的问题：要想扩大存储量，就必须牺牲速度。在存储量小的情况下，速度可以很快，但存储量一旦扩大，速度往往就会受到拖累。我们在日常生活中也经历过类似的事情——假如你有一张简短的历史购物单，要想在上面寻找某样东西，是一件轻而易举的事情，不需要花什么时间；但是，如果你写了二十年的日记，想在这些日记当中寻找某一天购买过的某样商品，那么寻找起来就会困难很多，花的时间也会长很多。因此，回到计算机的问题上，要想兼顾速度和存储量，可以结合采用多种不同类型的存储器。如今的计算机处理器通常有如下配置：少量超快内部存储器（称为寄存器）；一个内部高速缓冲存储器（cache，简称高速缓存）（或许还会加装一个容量稍大、但速度稍慢的高速缓存）；一个比高速缓存更慢，但却更大的外部存储器；一个比外部存储器还慢，但却大很多的硬盘；此外或许还会加装一个比硬盘还慢，但却更大的备份存储器（磁带或硬盘）。只要在时机把握得当的情况下，将数据和指令从慢速存储器转移到快速存储器，计算机就能迅速调取信息，从而运行得更快。（这就好比你在写日记的时候，将重要的信息提取出来，写在一张纸上，这样日后查找起来就会更方便。）现代计算机处理器还有一种典型的优化设计方案，称为流水线（pipelining，又称管线）。其具体执行过程非常类似于工厂中的流水线。下面我给大家具体解释一下。工厂制造一辆车可能需要一整天，但是几乎每时每刻都有新车出厂。这是因为采用了流水线操作，生产汽车的流程被分为许多道工序，所有工序并行操作，不同的车辆同时进入不 同的工序。比方说，生产汽车的部分工序依次包括：焊接门框，安装车门，安装电动车窗。第一辆车进入焊接工序的同时，第二辆车进入装门工序（比第一辆车早一道工序），第三辆车进入装窗工序（比第二辆车早一道工序）。接着，所有汽车通过运输带自动进入下一道工序：第一辆车进入装门工序，第二辆车进入装窗工序，同时，另有一辆车接替第一辆车，进入焊接工序。计算机处理器的流水线技术也是一样的道理：将一条或一组指令的执行过程拆分为多个步骤，然后通过硬件处理单元 尽可能多地并行执行这些步骤。处理器的流水线段数越多，在理论上可以并行执行的指令数也就越多。除了流水线作业以外，还有其他方法可以让计算机并行执行指令，比如采用向量处理器。向量处理器不仅可以执行向量计算（一次直接操作一维数组，而不仅仅是一个数据），还可以并行运行多个处理器。 在当今时代，就连个人计算机的处理器中，也包含人类有史以来最精巧繁复的设计作品。它们已经复杂到无以复加，以至于没有其他计算机的辅助，就不可能完成设计过程。无论是安排硅芯片上晶体管的布局架构，还是设计处理器的集成电路，这类低端的设计工作现在已经基本上没有人在做了。未来计算机的细节设计已经交给当今时代的计算机来承担。 并行化是计算机的未来计算机技术的进步看似永无止境，而且，摩尔定律可能会让人想当然地以为，处理器势必会一直朝着更小巧、更便宜、更快捷的方向发展下去。但是，任何事物都不可能永远保持飞速发展的状态。事实上，我们在前进的道路上已经遇到了一个障碍。计算机体系结构教授戴维·帕特森道出了其中的缘由：“过去十年到十五年的时间里，我们为了提升计算机的性能，不断地增加晶体管的数量。每一次增加晶体管的数量，都会使硅芯片的功耗和散热压力更大。每块芯片约100瓦的功耗已经是其散热能力的极限。我们大概在2003年达到了这个水平。要想继续利用摩尔定律提升计算机的性能，唯一的出路就是制造并行计算机。这就意味着我们必须改变编程模型， 这是六十年的计算发展史上最重大的变革。”也就是说，问题并不在于摩尔定律。因为在未来的许多年里，硅芯片上的晶体管数目似乎还会越来越多。真正的问题在于，一个体积较大、而且精密复杂的处理器会在运转的过程中发热。笔记本发烫到致人三度烧伤、处理器发烫到融化电路板，这种夸张的事情没有人愿意看到。唯一的解决办法是，制造更小、更简单的处理器，使单个处理器的功耗减小，同时在单个芯片上集成多个处理器。计算机处理器的设计理 念已然开始发生变迁。因此，现在市面上的计算机的时钟频率（clock speed）（用于衡量计算机解读新指令的速度）可能并没有快多少，但是处理器中的核数却增加了不少。不过这里面有一个问题：既然有了并行或多核处理器，就理应配备能够高效利用其性能的软件，但是编写这样的软件对于程序员来说，是一件极其困难的事情。如今，硬件条件已经齐备，但是软件设施明显跟不上前进的步伐，无法充分发挥计算机的全部运算能力。帕特森同意这一观点，他在最近发表的一篇文章中写道：“处理器的并行化和微型化是计算发展史上的一个里程碑。” 有许多计算机科学家试图尝试帕特森的方法，但是到目前为止，问题尚未解决，我们还不知道怎样编写并行软件，才能使之与新的计算机体系结构相适应。怎样才能写出并行文字处理器或并行电子邮件程序？这个问题着实令人头疼。不过，值得庆幸的是，有一些应用程序在本质上讲就已经是并行程序了。其中最明显的例子，或许莫过于我们在每一台现代计算机上都会看到的、神奇的计算机图形。计算机图形往往由成千上万个微小的多边形拼接而成，多边形的表面覆盖着照片质量的图像。打个比方，假如你想制作某件物品的三维模型，可以先用铁丝网将其形状构建出来，然后在表面覆盖图像，用以表现物品表面的图案。计算机图形也是一样的道理。无论是展示动画效果（比如游戏角色的动作、行驶在车道上的汽车），还是纯粹显示窗口和图标，计算机都在同时改变成千上万个多边形和图像的位置，而且每个多边形和图像的位置改变方式都非常相似。由此可见，计算机不费吹灰之力，就可以并行处理这些计算过程，提升运转速度。事实上，时至今日，图形的并行处理已经变得轻而易举，以至于大多数最先进的多核计算机结构都是图形处理单元（graphics processing units，简称GPU）。这些处理器已经有数百个内核，所有的内核都并行计算。因此，大多数游戏机、个人计算机、乃至小型便携式计算机都采用了并行GPU，以使图形流畅逼真。正因为GPU运算如此之快，内核如此之多，它已成为许多超级计算机的重要部件。处理器的并行化趋势并不仅仅表现在单个芯片上。云计算是最近出现的一个新概念。它提供了一个动态虚拟的架构，这个架构或许会改变我们对计算机的认识。有了它，计算机用户就可以购买处理时间，使用异地多台计算机的软件和存储器，而不需要知道提供服务的计算机位于何处、其部件究竟如何运转。从概念上讲，云计算将计算机视为一种资源——这跟水电的性质是一样的，我们在日常生活中都会用水用电，但却不需要知道自来水厂和发电站在哪里。云计算可以让用户使用最新软件，执行高强度计算，享受虚拟主机服务，而不需要参与异地物理主机的升级和维护。在线购书网站亚马逊就是云计算领域的创新先驱。该公司意识到，其庞大的数据中心通常只有10％的容量得到了有效利用，因此，在2006年，该公司开始推出亚马逊网络服务（Amazon Web Services），出售数据中心的闲置容量。其他公司可以按需购买亚马逊提供的任何计算服务、软件和存储空间，而不需要维护或升级任何计算机。这一做法已变得越来越受欢迎，很多公司将来或许会从“云端”购买 企业所需的一切计算服务，而不是大动干戈地建立和维护自己的内部计算机系统。另一方面，用户或许也可以通过为数众多的云端计算机执行并行处理，而不需要担心任务怎样拆分——只要云端软件足够智能，可以代劳就好。处理器的并行化趋势还体现在汽车上，这里的并行处理比较容易实现。一台现代汽车上可能会搭载一百多个微处理器，它们协同工作，共同确保发动机和传动装置的平稳运转，同时控制仪表板、车门锁、倒车雷达、车载收音机、GPS、车灯、后视镜、座椅调节器……事实上，大多数汽车都有自己的计算机网络，使车内不同的计算机能够高效配合。 在这一方面，读者朋友们或许已经有了亲身体验。当你踩下油门，发动机猛然加速时，车载收音机的音量也会自动变大。有些汽车甚至更加智能，可以将安全气囊加速计、停车灯、GPS导航系统、手机和车门锁进行互联，万一出了什么严重的事故，车子就会呼叫应急号码，将你的GPS坐标发送出去，同时解锁车门，打开停车灯。 超越冯·诺依曼不过，在有些人看来，如此惊人的技术进步依然远远不够。曼切斯特大学教授史蒂夫·弗伯（Steve Furber）在职业生涯的开始，曾为艾康（Acorn）计算机公司设计ARM 32位微处理器。这个设计在世界各地受到了热捧，如今，ARM内核的出货量已超过200亿个，比英特尔芯片的销量还多（ARM处理器本身也更昂贵）。多年来，全世界90％以上的手机都至少搭载了一个ARM内核。不过，弗伯虽然是正统的计算机设计师出身，但在1998年却决定改变方向。他意识到，生物大脑似乎比计算机处理器的计算和存储部件要优越许多。“最后，我一咬牙，就转方向了，”弗伯表示，“管他呢，反正我感兴趣的是神经网络。”如今，有越来越多的计算机设计师开始研究仿生计算机，弗伯就是其中的一员。他有一项雄心勃勃的设计，称为SpiNNaker。SpiNNaker是一个通用脉冲神经网络结构，其中包含成百上千个并行运转ARM微处理器。弗伯并不是唯一一个具备前沿思想的计算机科学家。如今，各种类型的仿生处理器正在紧锣密鼓的研制当中，它们的性能有的类似于大脑神经元，有的类似于免疫细胞，有的类似于细胞中的基因。有些研究人员甚至在尝试用新的材料代替硅，来制作处理信息的芯片。现在，还有诸多问题等着科学家来解答：我们能否在DNA链中存储信息，让它们跟基因一起重组？我们能否对细菌进行基因改造，从而发明新的计算机？我们能否发明量子计算机，让它根据匪夷所思的物理学量子效应来执行计算？约克大学（University of York）教授安迪·泰瑞尔是一名计算机工程师，专攻仿生计算机。他认为，在未来的一段时间里，计算机体系结构依然会采用经典的冯·诺依曼设计。“但是，你可以想象，有‘一小撮’计算机工程师已经在设计新的体系结构了（希望他们的设计不仅新奇，而且令人振奋）。研制过程中或许采取了新的材料（或材料组合），比如 分子器件、忆阻器（memresister）、生物电材料、各种化学结构（液晶已经被人试过了）。或许还有些新的材料我们根本就不知道。”泰瑞尔认为，自然界中一定隐藏着某些秘诀，能够启发我们改善计算机的性能。毕竟，自然界中存在许多复杂而又高度精巧的事物。“怎样才能在保持原有优势的基础上，将材料换成新的呢？这是一个难关。采用生物材料的系统似乎能搭载更多（也更复杂）的部件，其数量（和复杂度）可能会超乎人的想象。因此，我们面临的一大挑战或许是：怎样才能制造这样的系统？应该采用什么材料？怎样才能发挥它的性能？”]]></content>
      <categories>
        <category>计算机</category>
        <category>历史</category>
        <category>先驱</category>
      </categories>
      <tags>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机先驱——约翰·冯·诺依曼]]></title>
    <url>%2F2019%2F05%2F18%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%85%88%E9%A9%B12%2F</url>
    <content type="text"><![CDATA[John von Neumann，1903年12月28日—1957年2月8日1921年，冯·诺依曼在布达佩斯大学注册为数学方面的学生。与此同时，冯·诺依曼进入柏林大学（1921年），1923年又进入瑞士苏黎世联邦工业大学学习化学。1926年他在苏黎世联邦工业大学获得化学方面的大学毕业学位，通过在每学期期末回到布达佩斯大学通过课程考试，他也获得了布达佩斯大学数学博士学位。先后执教于柏林大学和汉堡大学，1930年前往美国，后入美国籍。历任普林斯顿大学、普林斯顿高级研究所教授，美国原子能委员会会员。美国全国科学院院士。20世纪最重要的数学家之一，在现代计算机、博弈论、核武器和生化武器等领域内的科学全才之一，被后人称为“计算机之父”和“博弈论之父”。 制造大脑事有凑巧，1943年，当图灵回到英国后，一位名叫约翰·冯·诺依曼的数学家也从美国漂洋过海，来到了英国。冯·诺依曼当时是普林斯顿高等研究院最年轻的成员（他是最先被研究院聘为教授的五人之一，同时入院的还有爱因斯坦），他人脉很广，也认识图灵，因为在1938年，图灵博士毕业，成为研究助理后，他曾要求图灵留在普林斯顿。但图灵拒绝了这个工作机会，回到了英国剑桥。冯·诺依曼也见过香农。那是在1940年，香农还在高级研究所担任研究员。 20世纪30年代，人们越来越热衷于发明自动计算机。几个世纪以来，机械式计算机除了齿轮和螺丝钉以外，什么部件也没有。冯·诺依曼对这些设备深为着迷，他尤其痴迷于19世纪20年代查尔斯·巴贝奇（Charles Babbage）发明的一台机械式计算机，它与如今的现代计算机在设计上有许多共通之处。随着继电器这种电控开关的问世，利用电动设备进行计算已经成为了可能。不少工程师发明了早期计算机。其中最早的机型之一采用了柏林工程师康拉德·楚泽（Konrad Zuse）发明的继电器。这台巨型机器称为Z3，由于缺少条件分支，它的功能受到了一定的局限。也就是说，它不能根据不同的计算结果执行不同的操作，必须在程序中不停地执行相同的计算。它的运行速度也很慢，因为继电器采用了移动部件接通和断开电流。运行速度更快的计算机都采用了没有移动部件的电子管（真空管）。20世纪30年代，美国爱荷华州立大学（Iowa State University）的 约翰·阿塔纳索夫（John Atanasoff）花了几年的时间发明了一台电子计算机，用来解线性代数方程。这台计算机由他和学生克利福德·贝里 （Clifford Berry）共同制造，称为“阿塔纳索夫-克利福德贝里计算机”（Atanasoff-Berry Computer，简称ABC）。它体型较小，性能不甚可靠，内含大约300个电子管。与此同时，英国工程师托马斯·弗劳尔斯（Thomas Flowers）于1934年在继电器的基础上发明了独创的开关系统，并于30年代末投入使用。开关系统采用了3000多个电子管，用于英国电话交换机和简单的数据处理。 1943年，图灵回国后，鼓动马克斯·纽曼接近弗劳尔斯，请他来布莱切利公园，帮忙改进他们在继电器的基础上设计出来的破译机。1943年底，弗劳尔斯发明了巨像I（Colossus I）——一台内含1600个电子管的电子计算机。巨像I后来一共制造了十台，每一台都包含2400个电子管，但它们不是通用机器，必须插入不同的电缆重新编程。 这时候，冯·诺依曼已结束英国之行，回到了美国。由于他名气很大，美国国防部邀请他参与曼哈顿计划（Manhattan Project，美国政府制造第一颗原子弹的计划），负责设计原子弹的爆炸外护层（explosive outer jacket）。要完成这项工作，必须进行大量复杂的数学计算。冯·诺依曼意识到，他需要一台全新的计算机，其性能必须远远超越当前所有的计算机。一次偶然的相遇注定将改变一切。1944年夏，冯·诺依曼在马里兰州的火车站遇到了曾任数学教授的赫尔曼·戈德斯坦中尉（Herman Goldstine）。戈德斯坦当时是宾夕法尼亚大学（University of Pennsylvania）摩尔电气工程学院（Moore School of Electrical Engineering）的军方联络人。在摩尔电气工程学院，一台令人惊叹的全新计算机正在紧锣密鼓的研制当中。时至今日，戈德斯坦依然对当年与冯·诺依曼交谈的情形记忆犹新：“话题很快就转移到了我的工作上。我说我正在关注一项开发任务，任务的目的是制造一台每秒钟可以运算333个乘法的电子计算机。这句话才说完，整个谈话的气氛一下子就变了，原本我们还是比较轻松、随意地聊聊天、打打趣，突然变得像是在 做数学博士学位的口头答辩。”冯·诺依曼马上做出安排，拜访了项目的设计师：约翰·莫奇利（John Mauchly）和约翰·埃克特（John Presper Eckert）。他们设计的是世界上第一台通用计算机：电子数字积分计算机（The Electronic Numerical Integrator and Computer）——简称“埃尼阿克”（ENIAC）。这个庞然大物尺寸为8×3×100英寸（约合2.4米×0.9米×30米），重约三十吨，包含17,000多个电子管和1500多个继电器，运算速度比以往的任何计算机都快。冯·诺依曼很快就开始定期造访摩尔电气工程学院，并受邀参与了埃尼阿克的设计项目。埃尼阿克的研制工作进展缓慢，因此，在研制工作完成之前，军方又布置了一个任务，要求再建造一台更快的计算机。埃尼阿克的后继者称为电子离散变量自动计算机（Electronic Discrete Variable Automatic Computer）——简称“埃德瓦克”（EDVAC），约翰尼·冯·诺依曼成为了设计团队的一员。埃德瓦克的设计持续了好几个月，涉及到很多新思想和新技术。挑战主要来自两大方面，一是保存数据的存储器，需要解决的问题是，数据能否存储在某种形式的雷达甚至电视显像管里？二是指令系统，这里需要考虑的是，有哪些功能是计算机应该具备的？1945年6月，冯·诺依曼撰写了一篇文章，对摩尔学院项目团队的设计理念进行了总结。这只是一份初稿，署名作者只有他一个人，但是文章的终稿版从未出炉。戈德斯坦鼓励将初稿的内容公诸于世，因此，埃德瓦克的设计思想很快在全世界研究人员和工程师中传播开来。文章标题为“关于埃德瓦克的报告初稿”（First Draft of a Report on the EDVAC）。这是第一份系统描述计算机制造方法的公开出版物，具有划时代的革新意义。 数字大脑的解剖尽管“关于埃德瓦克的报告初稿”描述的是工程学和数学领域的研究成果，但它的遣词造句通俗而又浅显，几乎人人都能看懂。报告是冯·诺依曼1945年在火车上手写的，以下是部分摘录：我们只要分析一下这台构想中的设备应该具备哪些功能，就不难看出，该设备的核心部件应该分为几个大类。第一：由于该设备本质上是一台计算机，最起码要能够迅速地执行基本的算术运算，其中包括加法、乘法和除法。由此可见，该设备应该包含专门执行此类运算的部件……也就是说，中央算术器（central arithmetic part）或许是一个必不可缺的部件，它也构成了该设备的第一大部件，我们将其简称为CA。第二：该设备需要对不同的运算操作进行适当的排序，这一过程称 为逻辑控制。中央控制器（central control organ）可以对设备进行高效的逻辑控制……它构成了该设备的第二大部件，我们将其简称为CC。第三：任何设备要想进行长时间的复杂操作（尤其是计算操作），都需要容量相当可观的存储器（memory）……它构成了该设备的第三大部件，我们将其简称为M。……CA、CC、M这三大部件就相当于人体神经系统中的联络神经元。至于哪些部件对应于人的感觉神经元（又称传入神经元）和运动神经元（又称传出神经元），是接下来要讨论的问题。这些部件是该设备的输入装置和输出装置。值得注意的是，这个设计方案包含五大逻辑元件：第一是中央算术器，负责执行所有的运算操作；第二是中央控制器，它决定机器的下一步动作；第三是存储器，用于保存程序及程序输出的结果；第四是输入设备，比如键盘；第五是输出设备，比如打印机。 算术逻辑单元（arithmetic and logic unit，简称ALU）和寄存器（register），用冯·诺依曼的说法就是中央算术器。这个电子装置接收外界输入的电信号，并据此输出各种各样的电信号（即运算结果）。ALU的运作模式固定不变，受到布尔逻辑电路的支配。它设有临时存储区域，称为寄存器，寄存器的运作方式和存储器一样（但是访问速度要快很多），用于保存ALU和存储器输出的结果。存储器。信息以二进制编码1和0的形式存储下来，二进制编码1和0则分别由“高”、“低”两种电压转换而来。在现代计算机中，这种类型的存储器通常被称为随机存取存储器（random access memory，简称RAM）。RAM这个缩写虽然看似神秘，但它的作用其实很简单。用通俗的话来讲，它可以帮助我们直接调取任何零散的信息，不需要像磁带或纸带那样，在庞杂的信息当中大海捞针。冯·诺依曼有很多设计理念（比如采用中央存储器，以保存数据和指令）明显受到了图灵机的影响。冯·诺依曼的同事、洛斯阿拉莫斯国家实验室（Los Alamos National Laboratory）的物理学家斯坦利·弗兰克尔（Stanley Frankel）深知图灵的工作在当时的重要性。“我知道，大约在1943-1944年这段时间，冯·诺依曼非常重视图灵在1936年写的论文……他让我看看这篇论文，于是我好好研究了一下。”冯·诺依曼也很重视香农的布尔逻辑，深知这一理论的重要价值。 他在报告中写道：这台机器的一大重要方面在于，它依据的不是算术原理，而是逻辑原理。非真即假的逻辑系统从本质上讲就是一个二进制的系统。 猝不及防的结束直到1951年，埃德瓦克才被制造出来。尽管它比埃尼阿克要小，但还是安装了6000个电子管（真空管），功率高达56千瓦，需要工业空调才能保持降温。没有人赶在战争结束之前成功造出可以使用的通用计算机。事实上，冯·诺依曼和最初的设计师莫奇利和埃克特之间发生了些许不快，因为莫奇利和埃克特对他们的理念不受重视而忿忿不平，他们想将自己的设计方案申请专利，投入商业使用。冯·诺依曼放弃了埃德瓦克，转而决定在高等研究院制造一台截然不同的计算机。受到此项工作的激发，很多其他计算机被相继设计出来。约翰·冯·诺依曼在高等研 究院开展的电子计算机项目（electronic computer project，简称ECP）比此前所有的项目都更加完善，整个系统运转速度极快，可以在不出现任何错误的情况下，长时间运行数据输入量和输出量都相当可观的程序（比如氢弹试验数据计算、天气预报、人工生命模拟）。ECP的成果和设计方案得到了广泛共享。IBM当年之所以能推出商业计算机（700系列），就是因为在合作参与ECP的过程中学到了关键技术。我们现在生活在冯·诺依曼构建的世界。现代的计算机系统虽然日新月异，但万变不离其宗，大框架都是冯·诺依曼1952年在普林斯顿构建起来的。 冯·诺依曼写下关于埃德瓦克的报告后，才过两年，贝尔实验室就发明了晶体管。从理论层面讲，晶体管能做到的事情和电子管完全相同，但从实际角度讲，晶体管具有无可比拟的优越性，不仅速度快了好几倍，而且耗电量和体积也小了很多。1953年，世界上第一台晶体管计算机由曼切斯特大学研究生迪克· 格里姆斯戴尔（Dick Grimsdale）制造完成。一石激起千层浪，此后15年里，世界各地涌现了一百多台晶体管计算机。但是，第一台晶体管计算机才问世五年，电子工程师就已经面临一个严峻的问题——“数字的暴政”（tyranny of numbers）：为了提高运算能力，就必须安装更多的晶体管，但是一旦安装更多的晶体管，制造和维护成本就会增大，因为每个部件都需要手工焊接到电路板上。这个问题并没有让电子工程师困扰多久。1958年，德州仪器（Texas Instruments）公司的新职员杰克·基尔比（Jack Kilby）产生了一个革命性的想法：为什么不同时制造所有的元件呢？只要蚀刻锗晶片，就可以将电路中所有的元件都集成到一小块芯片上。几个月后，到了1959年，另一位研究人员——罗伯特·诺伊斯（Robert Noyce）也独自想出了同样的主意，只不过他使用的不是锗晶片，而是硅晶片。集成电路（integrated circuit，简称IC），也就是硅芯片就此诞生。1957年，约翰·冯·诺依曼去世，时值图灵去世三年后，硅芯片问世两年前。]]></content>
      <categories>
        <category>计算机</category>
        <category>历史</category>
        <category>先驱</category>
      </categories>
      <tags>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机先驱——克劳德·艾尔伍德·香农]]></title>
    <url>%2F2019%2F05%2F18%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%85%88%E9%A9%B11%2F</url>
    <content type="text"><![CDATA[Claude Elwood Shannon，1916年4月30日—2001年2月24日1936年获得密歇根大学学士学位。1940年在麻省理工学院获得硕士和博士学位，1941年进入贝尔实验室工作。美国数学家、信息论的创始人。 时值1943年初，图灵正在访问他赴美之行的最后一站——贝尔实验室。他之所以来到这里，就是为了协助大西洋通信的语音加密工作（说白了就是给大西洋两岸传输的通话内容加密，这样敌人就无法监听）。不过，这次访问很快就因为另一个原因而变得收获颇丰。每天下午茶时间，图灵都会和实验室里的一位研究人员在食堂里长谈，这位研究人员名叫克劳德·香农（Claude Shannon）。两个人似乎都对计算机的问题非常热衷。图灵看问题主要从数学的视角出发，而香农的视角则完全不同。 逻辑思维香农进入贝尔电话实验室实习，学习操作自动电话交换机。实习期间，他意识到了一件重要的事情。年轻的香农发现，有两个看似截然不同的事物其实具有共同的本质。克劳德·香农回到麻省理工学院，发展他的理论新思想。那时候，他还没有和图灵见过面，图灵当时还是邱奇的学生，正在250英里（约合402公里）以外的普林斯顿大学攻读博士。两人共进午餐是六年以后的事情。 香农已经知道，数学上有一种逻辑代数系统，叫做布尔逻辑，它得名于英国数学家乔治·布尔（George Boole）。在布尔逻辑中，任何逻辑 表达式的计算结果都不是数值，而是“真”、“假”这两种真值。你只需要使用逻辑运算符“与”、“或”、“非”，就可以表达任何你想表达的逻辑语句。这个逻辑语句可以是一个英文句子，比如“在下雨且阴天或无 风的时候，我会带伞。”布尔逻辑可以让我们描述和操纵逻辑表达式，这就和我们通过数学函数来操纵数字是一样的道理（正如上一章所讲的那样，所有的数学问题都可以归结为逻辑问题）。香农取得的突破在于，他注意到，逻辑和开关电路具有共同的本质。他借鉴了布尔逻辑，并运用它来定义带有机电式继电器（电气开关）的电路。香农的理论表明，整个电路都可以用布尔逻辑表述出来，只要巧妙地运用逻辑表达式，就可以简化和改善电路设计。 也就是在这个时候，人们开始着手制造世界上第一台电子计算机。由此可见，只要能够运用逻辑数学表达式设计出简洁而高效的电路，就能创造巨大的实用价值。由于所有的数学问题都可以归结为逻辑问题，而逻辑问题又可以通过电气开关表现出来，香农的理论表明，人们可以设计专门的电机，用来计算任何可计算的数学函数。 香农决定将他的思想写成研究报告发表出来，研究报告的题目是“继电器和开关电路的符号分析”（A Symbolic Analysis of Relay and Switching Circuits）。这项成果给计算机科学领域带来了重大突破。香农很快以此为基础，完成了他在麻省理工学院的硕士论文。这篇论文受到了广泛的赞誉，人们说它“或许是本世纪最重要、也最有名的硕士论文”。24岁时，香农写了一篇博士论文，从代数学的角度描述遗传学和进化论。毕业后，他作为国家研究员（National Research Fellow），在声名远播的普林斯顿高等研究院（Institute for Advanced Study，简称IAS）工作了一年。在那里，他接触到了一些世界上最顶尖的人才，其中包括赫尔曼·外尔（Hermann Weyl）、阿尔伯特·爱因斯坦、库尔特·哥德尔和约翰·冯·诺依曼（John von Neumann）。1941年，香农进入贝尔电话实验室，继续扩充自己的理论思想。两年后，在贝尔实验室的食堂，图灵在与香农交谈的过程中大为振奋——他的声音不由自主地越来越大，引起周围的人纷纷侧目。香农的话让他看到了希望曙光——图灵机或许真的可以变成现实！临走前，图灵买了一本电路入门书，把它带到回国的船上，在危险四伏的海上航程中如饥似渴地阅读起来。 其他贡献香农理论的重要特征是熵（entropy）的概念，他证明熵与信息内容的不确定程度有等价关系。熵曾经是波尔兹曼在热力学第二定律引入的概念，我们可以把它理解为分子运动的混乱度。信息熵也有类似意义，例如在中文信息处理时，汉字的静态平均信息熵比较大，中文是9.65比特，英文是4.03比特。这表明中文的复杂程度高于英文，反映了中文词义丰富、行文简练，但处理难度也大。信息熵大，意味着不确定性也大。 众所周知，质量、能量和信息量是三个非常重要的量。 人们很早就知道用秤或者天平计量物质的质量，而热量和功的关系则是到了19世纪中叶，随着热功当量的明确和能量守恒定律的建立才逐渐清楚。能量一词就是它们的总称，而能量的计量则通过“卡、焦耳”等新单位的出现而得到解决。然而，关于文字、数字、图画、声音的知识已有几千年历史了。但是它们的总称是什么，它们如何统一地计量，直到19世纪末还没有被正确地提出来，更谈不上如何去解决了。20世纪初期，随着电报、电话、照片、电视、无线电、雷达等的发展，如何计量信号中信息量的问题被隐约地提上日程。 1928年哈特利（R.V. H. Harley）考虑到从D个彼此不同的符号中取出N个符号并且组成一个“词”的问题。如果各个符号出现的概率相同，而且是完全随机选取的，就可以得到D^N个不同的词。从这些词里取了特定的一个就对应一个信息量I。哈特利建议用NlogD这个量表示信息量，即I=NlogD。这里的log表示以10为底的对数。后来，1949年控制论的创始人维纳也研究了度量信息的问题，还把它引向热力学第二定律。但是就信息传输给出基本数学模型的核心人物还是香农。1948年香农长达数十页的论文“通信的数学理论”成了信息论正式诞生的里程碑。在他的通信数学模型中，清楚地提出信息的度量问题，他把哈特利的公式扩大到概率Pi不同的情况，得到了著名的计算信息熵H的公式： H=-&sum;PilogPi 如果计算中的对数log是以2为底的，那么计算出来的信息熵就以比特（bit）为单位。在计算机和通信中广泛使用的字节（Byte）、KB、MB、GB等词都是从比特演化而来。“比特”的出现标志着人类知道了如何计量信息量。香农的信息论为明确什么是信息量概念作出决定性的贡献。香农在进行信息的定量计算的时候，明确地把信息量定义为随机不定性程度的减少。这就表明了他对信息的理解：信息是用来减少随机不定性的东西。或香农逆定义：信息是确定性的增加。]]></content>
      <categories>
        <category>计算机</category>
        <category>历史</category>
        <category>先驱</category>
      </categories>
      <tags>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机先驱——艾伦·麦席森·图灵]]></title>
    <url>%2F2019%2F05%2F17%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%85%88%E9%A9%B1%2F</url>
    <content type="text"><![CDATA[Alan Mathison Turing，1912年6月23日－1954年6月7日1931年图灵进入剑桥大学国王学院，毕业后到美国普林斯顿大学攻读博士学位英国数学家、逻辑学家，被称为计算机科学之父，人工智能之父。 谜题关于数学漏洞的问题，我给大家举个例子。剑桥有位数学家——伯特兰·罗素（Bertrand Russell）发现了一个数学漏洞。此前罗素的工作已经取得了巨大的成功——他证明了所有数学问题都可以还原为逻辑问题，也就是说，所有数学发现都可以用逻辑表达式重新写出来。 这项工作是伟大的，因为它有助于我们了解数学赖以建立的所有基本真理。但是后来，罗素发现了一个问题。他发现了一个悖论——也就是看起来既正确又不正确的论断。数学家经常寻找悖论，因为你如果觉得某件事情既正确又不正确，那么你的想法肯定有漏洞。所以，通过这种方法可以将很多想法证伪。相比之下，罗素悖论的性质要严重许多，因为它似乎预示着，整个数学体系是有漏洞的。罗素悖论给我们出的难题是： 假设有一个集合A，它的所有子集都具有一个共同的性质P——它们不包含自身。问题是：集合A是否包含自身？ 首先，若A包含自身，则A是A的子集，那么A具有性质P，由性质P知A不包含A；其次，若A不包含A，也就是说A具有性质P，而A是由所有具有性质P的集合组成的，所以A包含A。就像理发师悖论一样，唯一说得通的解法是，集合A既包含自身，又不包含自身。这在逻辑上是不可能的。罗素悖论的提出之所以让数学家如临大敌，是因为它预示着数学的理论基础存在漏洞。几个世纪以来，数学思想和证明无不建立在一系列的基本真理之上。连加法和减法的运算法则都是运用集合和逻辑学加以证明的。但是罗素悖论表明，任何数学证明都不再可信。 罗素悖论还只是这一切的开端。1931年，在图灵攻读高级课程的四年前，有位数学家一劳永逸地证明了数学体系必定是不完备的。他的名 字叫库尔特·哥德尔（Kurt Gödel）。哥德尔的第一条定理可以通过类似的方式表述出来：G＝“本命题不可以由理论T证明。”如果命题G事实上可以由理论T证明，则理论T中存在一个自相矛盾的定理G，既然有自相矛盾的地方，那么理论T就是不完备的。也就是说，T要是完备的理论，就不可以证明G，但是这样一来，T就有证明不了的命题，也称不上是完备的理论了。于是，G所指的内容就是真的：G既无法得到证明，但又是真命题。由此可见，有些事物不管能否得到证明，都可以为真。 图灵在学校也学到了一个与此相关的前沿思想。这是由德国数学家大卫·希尔伯特（DavidHilbert）在1928年提出的挑战。这项挑战称为“判定问题”（Entscheidungs problem）。希尔伯特想知道的是，一个命题的真假能否自动判定。他的问题是，对于给定的数学语言，有没有什么方法或者程序可以让机器判定某件事情的真假，并将结果显示出来。虽然这听起来颇为实用，但真正的挑战在于：这种自动化的方法或机器是否有可能存在？自动判定简单的句子似乎并不是遥不可及的事情，但如果是用复杂的数学语言写成的高难度句子，是否仍有可能加以判定？这种万能的真理说明者是否真有可能存在？ 永不停机的图灵机当时没有任何机器能做到这一点，于是图灵构想了一台能做到这一点的机器。他想象的是一台理论计算机。一台能从纸带上读取信息的机器。根据即时读取的指令，机器可以将纸带左移、右移，或在纸带上读取信息、输出结果。虽然这台奇怪的新机器终究只是纸上谈兵的假想机，但是这已经足够了，因为图灵只是想从理论上解决希尔伯特提出的问题而已。或许颇具讽刺意味的是，图灵虽然提出了关于通用计算机的思想，但却并不急着证明他的机器可以解决判定问题。相反，他想证明判定问题不可能得到解决，进而说明有些问题在数学上根本不可判定。为了做到这一点，图灵首先假想他的小计算机正在根据纸带上的信号执行一个运算，接着他提出了一个问题：有没有什么方法可以判断这 台机器究竟是会陷入死循环，不停地计算下去；还是会停止计算，给出结果呢？ 图灵认为，要想判断他的机器会不会停机，那就需要再构造一台图灵机，以对第一台机器进行检测，因为他知道，他假想的机器在理论上 可以进行任何数学运算。于是他假想出第二台图灵机，如果检测到第一台图灵机永不停机，那么第二台机器就会停机，然后输出“不停机”；如 果检测到第一台图灵机停了机，那么第二台机器就会一直运转下去。现在，脑筋急转弯的地方来了。假如第二台机器反观自身，判断自己会不会停止计算，那会发生什么情况？图灵对此进行了设想，他突然发现了一个悖论：如果机器检测到自己会永不停机，那么它就会停机，然后输出“不停机”；如果机器检测到自己停了机，那么它就会一直运转下去。这在逻辑上是不可能的，由此证明，有些图灵机是不可判定的——我们永远也无法判断它们会不会停机。 顺便说一下，图灵当时设计这个图灵机，完全只是为了辅助他证明这个问题而已，这个机器是假想的，不存在的就像画一条辅助线。可是后来他又发现，虽然这个机器不能解决所有的问题，但确实能够解决很多问题，而且真的是可以造出来的。于是…… 图灵就成为了“计算机科学之父”。 P是否等于NP？P和NP指的是两种类型的问题，它们的计算复杂度各不相同。P类问题可以通过多项式时间算法解决。换句话说，凡是可以用O（n^x）算法解决的问题都是P类问题，不管这里的x是什么。排序问题就是典型的P类问题。就算是最好的排序算法，它的时间复杂度在最坏的情况下也是O（n^2），符合多项式关系，因此排序问题属于P类问题。对于NP类问题，我们可以在多项式时间内检验候选解是否正确，但是求解所需要的时间却会漫长许多——而且往往是指数时间。在已知量较小的情况下，所有这些问题乍看之下都很好解决，但是，一旦已知量的数量级增大，比如配送车穿行的城市增加到一百个、装箱的行李数量增加到五百个、硬币的数量限制增加到一百个，那么求解所需要的时间就会呈指数式增长。因此，P是否等于NP的问题实际上是在问：难度很大的NP类问题究竟能否用多项式算法求解？我们现在采用的算法是不是太愚蠢了，就像慢速排序那样？能不能找到像快速排序这种聪明的解法，让原本很难的问题一下子迎刃而解？ 显然，NP类问题很难解决，但早在1936年，图灵对如此艰涩的难题就已经有了自己的思考方法。思考NP类问题的方法之一，就是构造一台特殊的图灵机，称为非确定型图灵机（non-deterministic Turing Machine）。如果我们可以制造出这样的机器，就可以让它在多项式时间内运行NP类问题。之所以称之为非确定型，就是因为我们无法预测它的运作方式，但它总能找到最快的方法解决问题。试想你在干草堆里寻找一根针。你立马就能分辨出自己看到的是一棵干草还是一根针，但是从哪里寻找却是一个很大的问题。你有很大的选择空间，但问题是：“怎样做出选择才能让我找到解法？”非确定型图灵机的道理与之大同小异。它的问题是：“是否存在某种特定的指令，可以使我成功求解？”如果这样的指令存在，那么它就会惊呼“太好了！”然后遵照指令，在最快的时间内找到解法。如果这样的指令不存在，那么它就会唏嘘“太可惜了”，然后停止运转。至于这类聪明的图灵机是如何判断出解题方法的，这一点在某种程度上讲还是个迷。对此，人们设想了两种情况。第一，答案已经摆在那里了。这就好比你有一块魔镜，它无所不知，每次都会告诉你：这是最好的选择。第二，可以采用某种平行或并行操作，也就是说，这类非确定型图灵机所做的，其实就是同时运行所有可能的选择。 这些奇怪的思想是由图灵等业界先驱同时提出的，它们历经发展演进，为一个新的理论研究领域提供了肥沃的土壤，这个研究领域叫做可计算性理论（有时又称为递归论）。 总结可计算性理论以及图灵机图灵先知先觉，在电子计算机远未问世之前，他已经想到所谓“可计算性”的问题。物理学家阿基米得曾宣称:“给我足够长的杠杆和一个支点，我就能撬动地球。”类似的问题是，数学上的某些计算问题，是不是只要给数学家足够长的时间，就能够通过“有限次”的简单而机械的演算步骤而得到最终答案呢？这就是所谓“可计算性”问题，一个必须在理论上做出解释的数学难题。 “图灵机”是一个虚拟的“计算机”，完全忽略硬件状态，考虑的焦点是逻辑结构。图灵在他那篇著名的文章里，还进一步设计出被人们称为“通用图灵机”的模型，它可以模拟其他任何一台解决某个特定数学问题的“图灵机”的工作状态。他甚至还想象在带子上存储数据和程序。“通用图灵机”实际上就是现代通用计算机的最原始的模型。不过图灵在提出图灵机构想之后，又发现了新问题，有些问题图灵机是无法计算的。比如定义模糊的问题，如“人生有何意义”，或者缺乏数据的问题，“明天3D中奖号是多少”，其答案当然是无法计算出来的。但也有一些定义完美的计算问题，它们亦是不可解的，这类问题称为不可计算问题。不可计算的问题在践中几乎碰不到，事实上，很难找到这样的例子，既不可计算但又有人向计算的明确定义的问题。一个罕见的问题是所谓的停机问题。 图灵机的意义图灵提出图灵机的模型并不是为了给出计算机的设计，它的意义我认为有如下几点：1.它证明了通用计算理论，肯定了计算机实现的可能性，同时它给出了计算机应有的主要架构；2.图灵机模型引入了读写与算法与程序语言的概念，极大的突破了过去的计算机器的设计理念；3.图灵机模型理论是计算学科最核心的理论，因为计算机的极限计算能力就是通用图灵机的计算能力，很多问题可以转化到图灵机这个简单的模型来考虑。对图灵机给出如此高的评价并不是高估，因为从它的设计与运行中，我们可以看到其中蕴涵的很深邃的思想。通用图灵机等于向我们展示这样一个过程：程序和其输入可以先保存到存储带上，图灵机就按程序一步一步运行直到给出结果，结果也保存在存储带上。另外，我们可以隐约看到现代计算机主要构成（其实就是冯诺依曼理论的主要构成），存储器（相当于存储带），中央处理器（控制器及其状态，并且其字母表可以仅有0和1两个符号），IO系统（相当于存储带的预先输入）；]]></content>
      <categories>
        <category>计算机</category>
        <category>历史</category>
        <category>先驱</category>
      </categories>
      <tags>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdowm空格缩进、多层嵌套]]></title>
    <url>%2F2019%2F05%2F17%2Fmarkdowm%E7%A9%BA%E6%A0%BC%E7%BC%A9%E8%BF%9B%2F</url>
    <content type="text"><![CDATA[空格缩进 一个空格表示：&amp;ensp;或&amp;#8194; 两个空格表示：&amp;emsp;或&amp;#8195; 不换行空格：&amp;nbsp;或&amp;#160; 多层嵌套效果： 第一层 第二层 &amp;alpha; &amp;acute; 代码： 12345&gt; 第一层&gt;&gt; 第二层&gt;&gt; &gt;&gt; &amp;alpha;&gt;&gt; &amp;acute;]]></content>
      <categories>
        <category>书写方法</category>
      </categories>
      <tags>
        <tag>markdowm书写方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F05%2F14%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
